{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1O-_VvdXw4h"
   },
   "source": [
    "### Contenido del Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter Sentiment140: Tweets related to brands/keywords. Website includes papers and research ideas. (77 MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfUXi7sqXw4j"
   },
   "source": [
    "The data is a CSV with emoticons removed. Data file format has 6 fields:\n",
    "- **0** - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "- **1** - the id of the tweet (2087)\n",
    "- **2** - the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "- **3** - the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "- **4** - the user that tweeted (robotickilldozr)\n",
    "- **5** - the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puLzRFPyXw4m"
   },
   "source": [
    "If you use this data, please cite Sentiment140 as your source.\n",
    "- **Link** : http://help.sentiment140.com/for-students/\n",
    "- **CSV** : https://docs.google.com/file/d/0B04GJPshIjmPRnZManQwWEdTZjg/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WHdlF5c7Xw4o"
   },
   "source": [
    "## 1. Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dyCnDoXoXw4s",
    "outputId": "c62d29d2-ed47-43e8-b6ae-483856c87bec"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "header_names = [\"polarity\",\"id\",\"date\",\"query\",\"user\",\"tweet\"]\n",
    "df = pd.read_csv(\"trainingandtestdata/training.1600000.processed.noemoticon.csv\", sep=',', names = header_names, encoding=\"ISO-8859-1\",header=None)\n",
    "test = pd.read_csv(\"trainingandtestdata/testdata.manual.2009.06.14.csv\", sep=',', names = header_names, encoding=\"ISO-8859-1\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_size(df):\n",
    "    \"\"\"Return the size of a DataFrame in Megabyes\"\"\"\n",
    "    total = 0.0\n",
    "    for col in df:\n",
    "        total += df[col].nbytes\n",
    "    return total/1048576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de archivo: 73.2421875 MB\n"
     ]
    }
   ],
   "source": [
    "print('Tamaño de archivo:',df_size(df), 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "migD5PMXXw44"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity          id                          date     query  \\\n",
       "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5         0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6         0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7         0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8         0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9         0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "5         joy_wolf                      @Kwesidei not the whole crew   \n",
       "6          mybirch                                        Need a hug   \n",
       "7             coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "8  2Hood4Hollywood               @Tatiana_K nope they didn't have it   \n",
       "9          mimismo                          @twittera que me muera ?   "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos de que todas las columnas estar cargando correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity\n",
      "id\n",
      "date\n",
      "query\n",
      "user\n",
      "tweet\n"
     ]
    }
   ],
   "source": [
    "for x in df:\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = df['polarity'].values\n",
    "tweets = df['tweet'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "print (category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos funcion de carga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_datos():\n",
    "    import pandas as pd\n",
    "    header_names = [\"polarity\",\"id\",\"date\",\"query\",\"user\",\"tweet\"]\n",
    "    df = pd.read_csv(\"trainingandtestdata/training.1600000.processed.noemoticon.csv\", sep=',', names = header_names, encoding=\"ISO-8859-1\",header=None)\n",
    "    test = pd.read_csv(\"trainingandtestdata/testdata.manual.2009.06.14.csv\", sep=',', names = header_names, encoding=\"ISO-8859-1\",header=None)\n",
    "    category = df['polarity'].values\n",
    "    tweets = df['tweet'].values\n",
    "    return df,test,category,tweets\n",
    "#    df = pd.read_csv(\"trainingandtestdata/training.1600000.processed.noemoticon.csv\", sep=',', names = header_names, encoding=\"ISO-8859-1\",header=None,nrows = 10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    800000\n",
      "0    800000\n",
      "Name: polarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideramos válido el apóstrofe (') debido a que este es parte de varios *stopwords*, concepto que será explicado luego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tweets_alphanum=[]\n",
    "for i in tweets:\n",
    "    x = re.sub(r\"http\\S+|www\\S+\",\" \",i)\n",
    "    x = re.sub(r\"[^\\w'@ ]|@\\W+\",\" \",x)\n",
    "    x = re.sub(r\"^(\\d ?)+| (\\d ?)+\",\" \",x)\n",
    "    tweets_alphanum.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_lower=[]\n",
    "for i in tweets_alphanum:\n",
    "    tweets_lower.append(i.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los *stopwords* o palabras vacías son términos que por lo general no guardan mucha información relevante para el procesamiento de la información textual (si no se analiza a nivel contextual). Por ello, para este caso, se removerán los stopwords de los títulos. Indique la cantidad de stopwords que está removiendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords_eng = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de stopwords removidos: 8655098\n"
     ]
    }
   ],
   "source": [
    "tweets_lower_clean=[]\n",
    "count = 0\n",
    "for tweet in tweets_lower:\n",
    "    tweet_clean = ''\n",
    "    for token in tweet.split():\n",
    "        if token not in stopwords_eng:\n",
    "            tweet_clean += token\n",
    "            tweet_clean += ' '\n",
    "        else:\n",
    "            count += 1    \n",
    "        tweets_lower_clean.append(tweet_clean)\n",
    "print(\"Cantidad de stopwords removidos:\",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los tweets con solamente carácteres alfanuméricos en minuscula limpios de stopwords, se procederá a analizar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tokens mas comunes: [('get', 737660), ('good', 709135), ('quot', 706196), ('like', 697424), ('day', 689489), ('got', 675474), ('going', 643472), ('go', 611164), ('know', 526583), ('today', 521242)]\n",
      "Numero de tokens: 104233416\n",
      "Tamaño del vocabulario: 627975\n"
     ]
    }
   ],
   "source": [
    "import collections as co\n",
    "tokens_list = []\n",
    "c = co.Counter() \n",
    "\n",
    "for text in tweets_lower_clean:\n",
    "    tokens = text.split()\n",
    "    c.update(tokens)\n",
    "\n",
    "print('10 tokens mas comunes:',c.most_common(10))\n",
    "print('Numero de tokens:',sum(c.values()))\n",
    "print('Tamaño del vocabulario:',len(c))\n",
    "## Observacion: También se puede calcular el tamaño del vocabulario con\n",
    "## len(list(set(\" \".join(tweets_lower_clean).split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos funciones de preprocesamiento y limpieza de datos resumiendo todo el análisis previo, como también una función de cálculo de estádisticas según el diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocesar(tweets):\n",
    "    tweets_alphanum=[]\n",
    "    for i in tweets:\n",
    "        x = re.sub(r\"http\\S+|www\\S+\",\" \",i)\n",
    "        x = re.sub(r\"[^\\w'@ ]|@\\W+|[^a-zA-Z]'[^a-zA-Z]\",\" \",x)\n",
    "        x = re.sub(r\"^(\\d ?)+| (\\d ?)+\",\" \",x)\n",
    "        tweets_alphanum.append(x)\n",
    "    tweets_lower=[]\n",
    "    for tweet in tweets_alphanum:\n",
    "        tweets_lower.append(tweet.lower())\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    stopwords_eng = nltk.corpus.stopwords.words('english')\n",
    "    tweets_lower_clean=[]\n",
    "    for tweet in tweets_lower:\n",
    "        tweet_clean = ''\n",
    "        for token in tweet.split():\n",
    "            if token not in stopwords_eng:\n",
    "                tweet_clean += token + ' '\n",
    "        tweets_lower_clean.append(tweet_clean)\n",
    "    import collections as co\n",
    "    diccionario = co.Counter() \n",
    "    for text in tweets_lower_clean:\n",
    "        tokens = text.split()\n",
    "        diccionario.update(tokens)\n",
    "    return diccionario, tweets_lower_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estadisticas(diccionario):\n",
    "    print('10 tokens más comunes:',diccionario.most_common(10))        \n",
    "    print('Número de tokens:',sum(diccionario.values()))\n",
    "    print('Tamaño del vocabulario:',len(diccionario))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alulab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "counters, tweets_lower_clean = preprocesar(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tokens más comunes: [(\"i'm\", 128139), ('good', 89368), ('day', 84761), ('get', 81573), ('like', 77766), ('go', 72945), ('today', 64615), ('going', 64079), ('love', 63430), ('work', 62857)]\n",
      "Número de tokens: 12154320\n",
      "Tamaño del vocabulario: 808557\n"
     ]
    }
   ],
   "source": [
    "estadisticas(counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deben mostrar un reporte de clasificación (accuracy) con diversos algoritmos de aprendizaje supervisado tradicionales (adicionales a los mostrados en clase) usando los siguientes vectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "def clean_tokens_profesor(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "def clean_tokens(text):\n",
    "    filtered_tokens = set(\" \".join(text).split())\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1**. Bag of Words (booleano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ \"switchfoot httptwitpiccomyzl awww that's bummer shoulda got david carr third day \",\n",
       "       \"upset can't update facebook texting might cry result school today also blah \",\n",
       "       'kenichan dived many times ball managed save rest go bounds ', ...,\n",
       "       'ready mojo makeover ask details ',\n",
       "       'happy th birthday boo alll time tupac amaru shakur ',\n",
       "       'happy charitytuesday thenspcc sparkscharity speakinguphh '], \n",
       "      dtype='<U176')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import numpy as np\n",
    "text = np.array(tweets_lower_clean)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 10000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features = 10000)\n",
    "txd_matrix = vectorizer.fit_transform(text)\n",
    "txd_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2146)\t1\n",
      "  (0, 8756)\t1\n",
      "  (0, 1318)\t1\n",
      "  (0, 2138)\t1\n",
      "  (0, 3595)\t1\n",
      "  (0, 7779)\t1\n",
      "  (0, 1182)\t1\n",
      "  (0, 8712)\t1\n",
      "  (0, 568)\t1\n",
      "  (1, 868)\t1\n",
      "  (1, 242)\t1\n",
      "  (1, 8882)\t1\n",
      "  (1, 7542)\t1\n",
      "  (1, 7236)\t1\n",
      "  (1, 2012)\t1\n",
      "  (1, 5504)\t1\n",
      "  (1, 8694)\t1\n",
      "  (1, 2968)\t1\n",
      "  (1, 9280)\t1\n",
      "  (1, 1266)\t1\n",
      "  (1, 9296)\t1\n",
      "  (2, 3543)\t1\n",
      "  (2, 7227)\t1\n",
      "  (2, 7510)\t1\n",
      "  (2, 5270)\t1\n",
      "  :\t:\n",
      "  (1599994, 9472)\t1\n",
      "  (1599994, 3566)\t1\n",
      "  (1599994, 9773)\t1\n",
      "  (1599994, 8832)\t1\n",
      "  (1599995, 795)\t1\n",
      "  (1599995, 2875)\t1\n",
      "  (1599995, 9737)\t1\n",
      "  (1599995, 3064)\t1\n",
      "  (1599995, 7542)\t1\n",
      "  (1599996, 4369)\t1\n",
      "  (1599996, 6082)\t1\n",
      "  (1599996, 1850)\t1\n",
      "  (1599996, 3884)\t1\n",
      "  (1599997, 5251)\t1\n",
      "  (1599997, 5613)\t1\n",
      "  (1599997, 2300)\t1\n",
      "  (1599997, 458)\t1\n",
      "  (1599997, 7032)\t1\n",
      "  (1599998, 221)\t1\n",
      "  (1599998, 8698)\t1\n",
      "  (1599998, 846)\t1\n",
      "  (1599998, 968)\t1\n",
      "  (1599998, 3814)\t1\n",
      "  (1599998, 8832)\t1\n",
      "  (1599999, 3814)\t1\n"
     ]
    }
   ],
   "source": [
    "txd_matrix = CountVectorizer(binary=True,max_features = 10000).fit_transform(text)\n",
    "print(txd_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 10000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_matrix = TfidfVectorizer(max_features = 10000, use_idf=False).fit_transform(text)\n",
    "tfidf_matrix = TfidfVectorizer().fit_transform(text)\n",
    "tf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos función que nos devuelve la matriz Bag of Words (booleano)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bag_Of_Words(tweets_lower_clean, max_words_dictionary = None):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    import numpy as np\n",
    "    text = np.array(tweets_lower_clean)\n",
    "    txd_matrix = CountVectorizer(binary=True,max_features = max_words_dictionary).fit_transform(text)\n",
    "    return txd_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2** TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 569)\t0.297447998948\n",
      "  (0, 8710)\t0.235296305296\n",
      "  (0, 1181)\t0.355227854861\n",
      "  (0, 7779)\t0.418023218636\n",
      "  (0, 3593)\t0.190409828461\n",
      "  (0, 2137)\t0.334006343766\n",
      "  (0, 1317)\t0.481393667494\n",
      "  (0, 8754)\t0.380197072952\n",
      "  (0, 2145)\t0.176420743146\n",
      "  (1, 9293)\t0.312089210869\n",
      "  (1, 1265)\t0.193146011584\n",
      "  (1, 9277)\t0.292964831383\n",
      "  (1, 2965)\t0.292873668745\n",
      "  (1, 8692)\t0.366465402238\n",
      "  (1, 5502)\t0.257980594255\n",
      "  (1, 2011)\t0.291913346463\n",
      "  (1, 7235)\t0.37366234893\n",
      "  (1, 7540)\t0.228525503212\n",
      "  (1, 8878)\t0.177516669887\n",
      "  (1, 243)\t0.255267514947\n",
      "  (1, 870)\t0.345117596773\n",
      "  (2, 5292)\t0.339619681078\n",
      "  (2, 8835)\t0.347237270736\n",
      "  (2, 633)\t0.443687621568\n",
      "  (2, 5270)\t0.45806327023\n",
      "  :\t:\n",
      "  (1599994, 2773)\t0.355319278557\n",
      "  (1599994, 807)\t0.292498614727\n",
      "  (1599994, 4665)\t0.332436095793\n",
      "  (1599994, 907)\t0.3785636548\n",
      "  (1599995, 7540)\t0.418949004935\n",
      "  (1599995, 3062)\t0.439367601521\n",
      "  (1599995, 9735)\t0.48779613319\n",
      "  (1599995, 2873)\t0.453231123445\n",
      "  (1599995, 796)\t0.433675366124\n",
      "  (1599996, 3883)\t0.43217538643\n",
      "  (1599996, 1851)\t0.408237121\n",
      "  (1599996, 6076)\t0.423029266854\n",
      "  (1599996, 4369)\t0.683822438786\n",
      "  (1599997, 7031)\t0.294123526002\n",
      "  (1599997, 459)\t0.361195332997\n",
      "  (1599997, 2298)\t0.44363709603\n",
      "  (1599997, 5611)\t0.539715345105\n",
      "  (1599997, 5251)\t0.543067911184\n",
      "  (1599998, 8829)\t0.268112269235\n",
      "  (1599998, 3811)\t0.314432248966\n",
      "  (1599998, 969)\t0.417752972908\n",
      "  (1599998, 847)\t0.373652979825\n",
      "  (1599998, 8696)\t0.37986394453\n",
      "  (1599998, 222)\t0.608947830144\n",
      "  (1599999, 3811)\t1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1600000, 10000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_1 = TfidfVectorizer(max_features=10000).fit_transform(tweets_lower_clean)\n",
    "print(X_1)\n",
    "X_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos función que nos devuelve la matriz TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TF_IDF(tweets_lower_clean, max_words_dictionary = None):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    X_1 = TfidfVectorizer(stop_words='english',max_features=10000).fit_transform(tweets_lower_clean)\n",
    "    return (X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(category)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **3** Word Vectors (embeddings) de spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deben instalar el modelo \"en_core_web_lg\"\n",
    "#nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doc=nlp(u'this is a sentence.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos función que nos devuelve la matriz Word Vectors (embeddings) de spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Word_Vectors(tweets_lower_clean, max_words_dictionary = None):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se define una función de ayuda para ejecutar un modelo y que se imprima el resultado de clasificación (accuracy) mediante validación cruzada (k_fold=5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(clf, X, y):\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    print(\"%s accuracy: %0.2f (+/- %0.2f)\" % \\\n",
    "          (str(clf.__class__).split('.')[-1].replace('>','').replace(\"'\",''), \n",
    "          scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_models(X,y):\n",
    "    #run_model(LinearSVC(), X, y)\n",
    "    run_model(SGDClassifier(), X, y)\n",
    "    run_model(Perceptron(), X, y)\n",
    "    run_model(PassiveAggressiveClassifier(), X, y)\n",
    "    run_model(BernoulliNB(), X, y)\n",
    "    #run_model(MultinomialNB(), X, y)\n",
    "    #run_model(KNeighborsClassifier(), X, y)\n",
    "    #run_model(NearestCentroid(), X, y)\n",
    "    #run_model(RandomForestClassifier(n_estimators=100, max_depth=10), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora efectuamos la prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1**. Bag of Words (booleano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier accuracy: 0.76 (+/- 0.00)\n",
      "Perceptron accuracy: 0.69 (+/- 0.01)\n",
      "PassiveAggressiveClassifier accuracy: 0.69 (+/- 0.01)\n",
      "BernoulliNB accuracy: 0.76 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "text = np.array(tweets_lower_clean)\n",
    "txd_matrix = CountVectorizer(binary=True,max_features = 10000).fit_transform(text)\n",
    "run_models(txd_matrix,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2** TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier accuracy: 0.75 (+/- 0.01)\n",
      "Perceptron accuracy: 0.68 (+/- 0.01)\n",
      "PassiveAggressiveClassifier accuracy: 0.72 (+/- 0.01)\n",
      "BernoulliNB accuracy: 0.75 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "X_1 = TfidfVectorizer(stop_words='english',max_features=10000).fit_transform(tweets_lower_clean)\n",
    "run_models(X_1,y)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "LAB 9 - GRUPO 8.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
