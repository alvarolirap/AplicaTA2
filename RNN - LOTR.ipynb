{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN y Character-level Neural LM (LOTR)\n",
    "Basado en: https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/\n",
    "<br><br>El cuaderno de trabajo presenta una aplicación directa de un **Character-Level Neural Language Model** para generar automáticamente texto en base a un archivo/documento textual de entrada. En este caso, se usaron los libros de LOTR (The Lord of the Rings)\n",
    "\n",
    "<br>**Se recomienda generar un notebook nuevo, colocando sólo las instrucciones útiles para su prueba de generación de textos. Modifique y ordene lo que considere conveniente para una mayor legibilidad y comprensión de su prueba (para la revisión con los JPs en el Lab. 10)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación (_utils_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definición de argumentos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Archivo de texto \n",
    "DATA_DIR = \"./lotr.txt\" \n",
    "#Modificar BATCH_SIZE o HIDDEN_DIM en caso tengan problemas de memoria\n",
    "BATCH_SIZE = 50 \n",
    "HIDDEN_DIM = 250 #500\n",
    "#Parametro para longitud de secuencia a analizar\n",
    "SEQ_LENGTH = 50 \n",
    "#Parametro para cargar un pesos previamente entrenados (checkpoint)\n",
    "WEIGHTS = '' \n",
    "\n",
    "#Parametro para indicar cuantos caracteres generar en cada prueba\n",
    "GENERATE_LENGTH = 500 \n",
    "#Parametros para la red neuronal\n",
    "LAYER_NUM = 2 \n",
    "NB_EPOCH = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función A:\n",
    "<br>(1) Carga de un archivo de texto, (2) Construcción de estructuras de entrada y salida de la red**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for preparing the training data\n",
    "def load_data(data_dir, seq_length):\n",
    "    #Carga del archivo\n",
    "    data = open(data_dir, 'r').read()\n",
    "    #Caracteres unicos\n",
    "    chars = list(set(data))\n",
    "    VOCAB_SIZE = len(chars)\n",
    "\n",
    "    print('Data length: {} characters'.format(len(data)))\n",
    "    print('Vocabulary size: {} characters'.format(VOCAB_SIZE))\n",
    "    print(chars)\n",
    "    \n",
    "    #Indexacion de los caracteres\n",
    "    ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "    char_to_ix = {char:ix for ix, char in enumerate(chars)}\n",
    "    \n",
    "    #Estructuras de entrada y salida\n",
    "    NUMBER_OF_SEQ = int(len(data)/seq_length)\n",
    "    print('Number of sequences: {}'.format(NUMBER_OF_SEQ))\n",
    "    X = np.zeros((NUMBER_OF_SEQ, seq_length, VOCAB_SIZE))\n",
    "    y = np.zeros((NUMBER_OF_SEQ, seq_length, VOCAB_SIZE))\n",
    "    \n",
    "    for i in range(0, NUMBER_OF_SEQ):\n",
    "        #LLenado de la estructura de entrada X\n",
    "        X_sequence = data[i*seq_length:(i+1)*seq_length]\n",
    "        X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n",
    "        #one-hot-vector (input)\n",
    "        input_sequence = np.zeros((seq_length, VOCAB_SIZE))  \n",
    "        #uso del diccionario para completar el one-hot-vector\n",
    "        for j in range(seq_length):\n",
    "            input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "            X[i] = input_sequence\n",
    "            \n",
    "        #Llenado de la estructura de salida y\n",
    "        y_sequence = data[i*seq_length+1:(i+1)*seq_length+1]\n",
    "        y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
    "        #one-hot-vector (output)\n",
    "        target_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "        #uso del diccionario para completar el one-hot-vector\n",
    "        for j in range(seq_length):\n",
    "            target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "            y[i] = target_sequence\n",
    "            \n",
    "    return X, y, VOCAB_SIZE, ix_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función B:\n",
    "<br>Generación de textos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for generating text\n",
    "def generate_text(model, length, vocab_size, ix_to_char):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y Prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uso de la Función A: carga de los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 3262172 characters\n",
      "Vocabulary size: 99 characters\n",
      "['2', 'r', 'P', 'U', 'j', 'ó', 'Q', 'o', '6', 'A', 'M', 'h', 'z', 'V', '_', '*', 'S', '–', 'X', ':', '!', 'F', '¢', '-', 'L', '=', '‘', 'B', 'm', 'i', 'W', '®', 'e', 's', '8', 'N', 'x', 'G', 'D', '«', '’', 'y', '0', ')', '4', 'K', 'l', '.', '…', '\\n', 'T', 't', 'n', 'R', 'O', 'q', '¤', 'u', 'Y', 'J', 'Z', '‚', 'p', 'g', '`', '¥', 'C', '—', '\"', 'I', '<', '(', 'k', 'E', ';', '7', 'd', '1', '9', '5', '#', 'w', ' ', '&', 'f', '/', '?', '}', 'v', '3', \"'\", '»', 'µ', 'b', 'a', ',', '>', 'c', 'H']\n",
      "Number of sequences: 65243\n"
     ]
    }
   ],
   "source": [
    "# Creating training data\n",
    "X, y, VOCAB_SIZE, ix_to_char = load_data(DATA_DIR, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Es importante guardar el diccionario `ix_to_char` en un archivo binario. Este debe ser cargado cada vez que se quiera retomar el entrenamiento o generar texto a partir de un checkpoint, debido a que el orden de los caracteres en el diccionario podría modificarse (no es un orden fijo)**\n",
    "<br>**NO MODIFICAR ESTE PICKLE AL REINICIAR EL NOTEBOOK PARA PROBAR CHECKPOINTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No modificar el pickle al reiniciar el cuaderno de trabajo para probar checkpoints previos\n",
    "with open('ix_to_char.pickle', 'wb') as handle:\n",
    "    pickle.dump(ix_to_char, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '2', 1: 'r', 2: 'P', 3: 'U', 4: 'j', 5: 'ó', 6: 'Q', 7: 'o', 8: '6', 9: 'A', 10: 'M', 11: 'h', 12: 'z', 13: 'V', 14: '_', 15: '*', 16: 'S', 17: '–', 18: 'X', 19: ':', 20: '!', 21: 'F', 22: '¢', 23: '-', 24: 'L', 25: '=', 26: '‘', 27: 'B', 28: 'm', 29: 'i', 30: 'W', 31: '®', 32: 'e', 33: 's', 34: '8', 35: 'N', 36: 'x', 37: 'G', 38: 'D', 39: '«', 40: '’', 41: 'y', 42: '0', 43: ')', 44: '4', 45: 'K', 46: 'l', 47: '.', 48: '…', 49: '\\n', 50: 'T', 51: 't', 52: 'n', 53: 'R', 54: 'O', 55: 'q', 56: '¤', 57: 'u', 58: 'Y', 59: 'J', 60: 'Z', 61: '‚', 62: 'p', 63: 'g', 64: '`', 65: '¥', 66: 'C', 67: '—', 68: '\"', 69: 'I', 70: '<', 71: '(', 72: 'k', 73: 'E', 74: ';', 75: '7', 76: 'd', 77: '1', 78: '9', 79: '5', 80: '#', 81: 'w', 82: ' ', 83: '&', 84: 'f', 85: '/', 86: '?', 87: '}', 88: 'v', 89: '3', 90: \"'\", 91: '»', 92: 'µ', 93: 'b', 94: 'a', 95: ',', 96: '>', 97: 'c', 98: 'H'}\n"
     ]
    }
   ],
   "source": [
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65243, 50, 99) (65243, 50, 99) 99\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creación de la RNN (LSTM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and compiling the Network\n",
    "model = Sequential()\n",
    "\n",
    "#Añadiendo las capas LSTM\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "#Añadiendo la operacion de salida\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#\"Compilando\" = instanciando la RNN con su función de pérdida y optimización\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prueba inicial de creación de 500 caracteres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eII555WZZ5ZII5Z5ZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'eII555WZZ5ZII5Z5ZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5ZII5ZZII55ZZIIZ5I5ZZII5Z5ZIIZ5IZ5Z'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate some sample before training to know how bad it is!\n",
    "generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se cargan los pesos (y el diccionario de los one-hot-vectors) en caso haya habido un entrenamiento previo**\n",
    "<br>WEIGHTS debe tener el valor del nombre del archivo de \"checkpoint\" guardado. Por ejemplo:\n",
    "<br>```WEIGHTS = \"checkpoint_layer_2_hidden_250_epoch_60.hdf5\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan los pesos de un entrenamiento previo (si se desea restaurar una ejecucion)\n",
    "#Se calcula el numero de epocas en base al nombre del archivo\n",
    "#Se carga el diccionario de caracteres (one-hot-vectors) para la generacion\n",
    "if not WEIGHTS == '':\n",
    "    model.load_weights(WEIGHTS)\n",
    "    nb_epoch = int(WEIGHTS[WEIGHTS.rfind('_') + 1:WEIGHTS.find('.')])\n",
    "    with open('ix_to_char.pickle', 'rb') as handle:\n",
    "        ix_to_char = pickle.load(handle)\n",
    "else:\n",
    "    #Si se va a empezar de 0:\n",
    "    nb_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ENTRENAMIENTO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aoncevay/.conda/envs/chana/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 93s 6ms/step - loss: 2.4602\n",
      "1 the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the stor\n",
      "\n",
      "Epoch: 1\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 94s 6ms/step - loss: 1.8765\n",
      "the stood and the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the st\n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 94s 6ms/step - loss: 1.6646\n",
      "The was a stranger of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great \n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 94s 6ms/step - loss: 1.5435\n",
      "®re the mountains of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the ma\n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 94s 6ms/step - loss: 1.4673\n",
      "Lead and strange the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the sta\n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 93s 6ms/step - loss: 1.4139\n",
      ", and the hobbits had been so that he had not so for a moment to the horse of the Entwives of the horse of the Entwives of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of \n",
      "\n",
      "Epoch: 6\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 92s 6ms/step - loss: 1.3736\n",
      "y the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the ston\n",
      "\n",
      "Epoch: 7\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 92s 6ms/step - loss: 1.3399\n",
      "K he was the stars of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of th\n",
      "\n",
      "Epoch: 8\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 91s 6ms/step - loss: 1.3114\n",
      "X the wind of the woods of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great sto\n",
      "\n",
      "Epoch: 9\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 93s 6ms/step - loss: 1.2864\n",
      "8 he said the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of \n",
      "\n",
      "Epoch: 10\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 91s 6ms/step - loss: 1.2636\n",
      ") and the light of the water and the light of the stream. The water of the Enemy in the dark horses of the trees. The water of the Enemy in the dark horses of the trees. The water of the Enemy in the dark horses of the trees. The water of the Enemy in the dark horses of the trees. The water of the Enemy in the dark horses of the trees. The water of the Enemy in the dark horses of the trees. The water of the Enemy in the dark horses of the trees. The water of the Enemy in the dark horses of the t\n",
      "\n",
      "Epoch: 11\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 90s 5ms/step - loss: 1.2427\n",
      "“ he saw that the stars of the stair was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a grea"
     ]
    }
   ],
   "source": [
    "# Training if there is no trained weights specified\n",
    "\n",
    "#Esta es la iteración importante\n",
    "#Pueden cambiar la condición para que termine en un determinado numero de epochs.\n",
    "while True:\n",
    "    print('\\n\\nEpoch: {}\\n'.format(nb_epoch))\n",
    "    #Ajuste del modelo, y entrenamiento de 1 epoca\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=1)\n",
    "    nb_epoch += 1\n",
    "    #Generacion de un texto al final de la epoca\n",
    "    generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "    #Pueden modificar esto para tener más checkpoints\n",
    "    if nb_epoch % 10 == 0:\n",
    "        model.save_weights('checkpoint_layer_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRUEBA errónea de un checkpoint anterior**\n",
    "<br>Al reiniciar el notebook, y cargar un checkpoint sin cargar correctamente el diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'checkpoint_layer_2_hidden_250_epoch_10_run1.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0618af1d9357>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mWEIGHTS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"checkpoint_layer_2_hidden_250_epoch_10_run1.hdf5\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Loading the trained weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWEIGHTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGENERATE_LENGTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mix_to_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[0;32m   1169\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'`load_weights` requires h5py.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1171\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1172\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'layer_names'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'model_weights'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m                 \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'checkpoint_layer_2_hidden_250_epoch_10_run1.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "WEIGHTS = \"checkpoint_layer_2_hidden_250_epoch_10_run1.hdf5\"\n",
    "# Loading the trained weights\n",
    "model.load_weights(WEIGHTS)\n",
    "generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "print('\\n\\n')\n",
    "\n",
    "#El ejemplo impreso en realidad es usando un \"diccionario\" re-ejecutado y no cargado desde el archivo\n",
    "#Por eso hay inconsistencias en los caracteres (el mapeo se ha desordenado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Segundo intento de ENTRENAMIENTO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 0\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 48s 3ms/step - loss: 2.7454\n",
      "un the he the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "\n",
      "Epoch: 1\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 2.1540\n",
      "/ the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the stor\n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.9082\n",
      "2 the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the \n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.7526\n",
      "But the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang t\n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.6431\n",
      "ut the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat th\n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.5638\n",
      "Eng of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the\n",
      "\n",
      "Epoch: 6\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.5047\n",
      "Come the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the s\n",
      "\n",
      "Epoch: 7\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.4575\n",
      "Ring the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the s\n",
      "\n",
      "Epoch: 8\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.4196\n",
      "6 the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of th\n",
      "\n",
      "Epoch: 9\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 48s 3ms/step - loss: 1.3875\n",
      "Ores and the streams of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of \n",
      "\n",
      "Epoch: 10\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.3600\n",
      "he stood the stars of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone \n",
      "\n",
      "Epoch: 11\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.3357\n",
      "Ught and the stars of the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and th\n",
      "\n",
      "Epoch: 12\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.3139\n",
      "And the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the sh\n",
      "\n",
      "Epoch: 13\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 45s 3ms/step - loss: 1.2939\n",
      "2 the stair of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stone\n",
      "\n",
      "Epoch: 14\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 45s 3ms/step - loss: 1.2748\n",
      "were the strange things were already from the streams of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stone\n",
      "\n",
      "Epoch: 15\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.2572\n",
      "Sam stood up and stood and strong and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and \n",
      "\n",
      "Epoch: 16\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.2403\n",
      "ze the stones of the trees and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong \n",
      "\n",
      "Epoch: 17\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.2241\n",
      "I should like to be seen them to the ground and the stars of the Ents and the others of the Ents and the others of the Ents and the others of the Ents and the stars and the stars are coming on the ground and the stars are the stars and the stars are the stars and the stars are the stars and the stars are the stars and the stars are the stars and the stars are the stars and the stars are the stars and the stars are the stars and the stars are the stars and the stars are the stars and the stars ar\n",
      "\n",
      "Epoch: 18\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.2086\n",
      "perhaps we shall see the way of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the st\n",
      "\n",
      "Epoch: 19\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.1933\n",
      "Not the hobbits saw that they had not seen the hills and the stairs of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stone\n",
      "\n",
      "Epoch: 20\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.1781\n",
      "e was a great deal of the wood, and then he stood up and stroke away from the rock of the stone with a strange place where the wood was still slowly in the stony slopes of the stones, and some sleep strange packs of the stones of the forest of the wood, and the world of the world came to the ground. The stones were great as a strange sword and shadow of the wood and strange them to the stream the shadow of the forest of the wood and strange them to the stream the shadow of the forest of the wood\n",
      "\n",
      "Epoch: 21\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.1638\n",
      ") the stars were still slowly in the stars and the shadow of the stony hall of the star of the stony wall and then suddenly they were already better than the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mo\n",
      "\n",
      "Epoch: 22\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.1495\n",
      "The mountains of the trees and the storm of the story was still strangely there was a great stone slopes of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of th\n",
      "\n",
      "Epoch: 23\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.1351\n",
      "_ the walls of Mordor, and the water ran up the stream that he had not seen the wind of a long shadow of the wall, and they came and stranger of the stream that he had not seen the wind of a long shadow of the wall, and they came and stranger of the stream that he had not seen the wind of a long shadow of the wall, and they came and stranger of the stream that he had not seen the wind of a long shadow of the wall, and they came and stranger of the stream that he had not seen the wind of a long s\n",
      "\n",
      "Epoch: 24\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.1210\n",
      "\\ the stream that he seemed to be seen on the ground and crawled away, and the light was close at hand. The horn of the mountains were still and stream that he was a great strange pale and stream the stream that he seemed to be seen on the ground and crawled away, and the light was close at hand. The horn of the mountains were still and stream that he was a great strange pale and stream the stream that he seemed to be seen on the ground and crawled away, and the light was close at hand. The horn\n",
      "\n",
      "Epoch: 25\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.1075\n",
      "But the sound of the world should have been the wild of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones\n",
      "\n",
      "Epoch: 26\n",
      "\n",
      "Epoch 1/1\n",
      "12450/16443 [=====================>........] - ETA: 11s - loss: 1.0898"
     ]
    }
   ],
   "source": [
    "nb_epoch = 0\n",
    "while True:\n",
    "    print('\\n\\nEpoch: {}\\n'.format(nb_epoch))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=1)\n",
    "    nb_epoch += 1\n",
    "    generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "    if nb_epoch % 10 == 0:\n",
    "        model.save_weights('checkpoint_layer_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRUEBA correcta de un checkpoint anterior**\n",
    "<br>Si instancian el modelo y sus parametros (ejecutando algunas celdas preliminares), y tienen los 2 archivos requeridos (.pickle y .hdf5) pueden generar el texto. \n",
    "<br>En el ejemplo de LOTR: `VOCAB_SIZE = 84` (si desean probarlo, se adjuntar los pesos y el diccionario, pero no el texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension 0 in both shapes must be equal, but are 99 and 84. Shapes are [99,1000] and [84,1000]. for 'Assign_2' (op: 'Assign') with input shapes: [99,1000], [84,1000].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1566\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1567\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1568\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Dimension 0 in both shapes must be equal, but are 99 and 84. Shapes are [99,1000] and [84,1000]. for 'Assign_2' (op: 'Assign') with input shapes: [99,1000], [84,1000].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-3ccfe0ba9f41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mWEIGHTS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"checkpoint_layer_2_hidden_250_epoch_60.hdf5\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Loading the trained weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWEIGHTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGENERATE_LENGTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mix_to_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[0;32m   1178\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[1;32m-> 1180\u001b[1;33m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[0;32m   1181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers, reshape)\u001b[0m\n\u001b[0;32m    927\u001b[0m                              ' elements.')\n\u001b[0;32m    928\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   2428\u001b[0m                 assign_placeholder = tf.placeholder(tf_dtype,\n\u001b[0;32m   2429\u001b[0m                                                     shape=value.shape)\n\u001b[1;32m-> 2430\u001b[1;33m                 \u001b[0massign_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2431\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2432\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking)\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0massignment\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m     \"\"\"\n\u001b[1;32m--> 615\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[0;32m    281\u001b[0m     return gen_state_ops.assign(\n\u001b[0;32m    282\u001b[0m         \u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    284\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m     58\u001b[0m         \u001b[1;34m\"Assign\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         use_locking=use_locking, name=name)\n\u001b[0m\u001b[0;32m     60\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    788\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   3390\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3391\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3392\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3394\u001b[0m       \u001b[1;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1732\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1733\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1734\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1735\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1736\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1568\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1569\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1570\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 99 and 84. Shapes are [99,1000] and [84,1000]. for 'Assign_2' (op: 'Assign') with input shapes: [99,1000], [84,1000]."
     ]
    }
   ],
   "source": [
    "#Cuidar de no reemplazar el pickle original\n",
    "with open('ix_to_char.pickle', 'rb') as handle:\n",
    "    ix_to_char = pickle.load(handle)\n",
    "    \n",
    "WEIGHTS = \"checkpoint_layer_2_hidden_250_epoch_60.hdf5\"\n",
    "# Loading the trained weights\n",
    "model.load_weights(WEIGHTS)\n",
    "generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
