{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el presente laboratorio se solicitó entrenar un *Character-level Languaje Model* basado en *Recurrent Neural Network* (RNN) sobre un conjunto de textos, que en este caso es el libro *War and Peace* del escritor ruso Leo Tolstoy, publicada en 1869.\n",
    "\n",
    "El objetivo del laboratorio es usar el modelo generado para predecir y generar texto completamente nuevo en base del original, para su posterior análisis.\n",
    "\n",
    "Dicho libro se encontrará en formato **.txt** con un peso de 3.3 MB, siguiendo la sugerencia de tener como mínimo un tamaño de 2 MB, para poder generar un modelo aceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recurrent Neural Network (RNN): https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/\n",
    "* DataSet: https://cs.stanford.edu/people/karpathy/char-rnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice\n",
    "1. Instalación de Librerías\n",
    "2. Procesamiento\n",
    "3. Entrenamiento y Prueba\n",
    "4. Generación de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución\n",
    "\n",
    "* Desde 0: Puntos 1-4\n",
    "* Entrenar: Puntos 1, 2, **Entrenamiento** de 3, 4\n",
    "* Generar texto: Puntos 1, 2 y 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Instalación de Librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando Anaconda Prompt se debe usar los siguientes comandos para importar la librería de Keras. Entrar en modo administrador a Anaconda Prompt e introducir los siguientes comandos.\n",
    "\n",
    "```conda update conda ```\n",
    "<br>```conda install keras ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar los siguientes comandos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install cython --user\n",
    "#!pip install --force-reinstall regex==2017.04.5\n",
    "#!pip install pathlib --user\n",
    "#!pip install msgpack --user\n",
    "!pip install tensorflow-gpu --user\n",
    "!pip install keras --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebo la correcta importación de librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TF!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant(\"Hello, TF!\")\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "print(sess.run(a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Archivo de texto \n",
    "DATA_DIR = \"./warpeace_input.txt\" \n",
    "#Modificar BATCH_SIZE o HIDDEN_DIM en caso tengan problemas de memoria\n",
    "BATCH_SIZE = 50 \n",
    "HIDDEN_DIM = 250 #500\n",
    "#Parametro para longitud de secuencia a analizar\n",
    "SEQ_LENGTH = 50 \n",
    "#Parametro para cargar un pesos previamente entrenados (checkpoint)\n",
    "WEIGHTS = '' \n",
    "\n",
    "#Parametro para indicar cuantos caracteres generar en cada prueba\n",
    "GENERATE_LENGTH = 500 \n",
    "#Parametros para la red neuronal\n",
    "LAYER_NUM = 2 \n",
    "NB_EPOCH = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función A\n",
    "(1) Carga de un archivo de texto, (2) Construcción de estructuras de entrada y salida de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for preparing the training data\n",
    "def load_data(data_dir, seq_length):\n",
    "    #Carga del archivo\n",
    "    data = open(data_dir, 'r').read()\n",
    "    #Caracteres unicos\n",
    "    chars = list(set(data))\n",
    "    VOCAB_SIZE = len(chars)\n",
    "\n",
    "    print('Data length: {} characters'.format(len(data)))\n",
    "    print('Vocabulary size: {} characters'.format(VOCAB_SIZE))\n",
    "    print(chars)\n",
    "    \n",
    "    #Indexacion de los caracteres\n",
    "    ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "    char_to_ix = {char:ix for ix, char in enumerate(chars)}\n",
    "    \n",
    "    #Estructuras de entrada y salida\n",
    "    NUMBER_OF_SEQ = int(len(data)/seq_length)\n",
    "    print('Number of sequences: {}'.format(NUMBER_OF_SEQ))\n",
    "    X = np.zeros((NUMBER_OF_SEQ, seq_length, VOCAB_SIZE))\n",
    "    y = np.zeros((NUMBER_OF_SEQ, seq_length, VOCAB_SIZE))\n",
    "    \n",
    "    for i in range(0, NUMBER_OF_SEQ):\n",
    "        #LLenado de la estructura de entrada X\n",
    "        X_sequence = data[i*seq_length:(i+1)*seq_length]\n",
    "        X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n",
    "        #one-hot-vector (input)\n",
    "        input_sequence = np.zeros((seq_length, VOCAB_SIZE))  \n",
    "        #uso del diccionario para completar el one-hot-vector\n",
    "        for j in range(seq_length):\n",
    "            input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "            X[i] = input_sequence\n",
    "            \n",
    "        #Llenado de la estructura de salida y\n",
    "        y_sequence = data[i*seq_length+1:(i+1)*seq_length+1]\n",
    "        y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
    "        #one-hot-vector (output)\n",
    "        target_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "        #uso del diccionario para completar el one-hot-vector\n",
    "        for j in range(seq_length):\n",
    "            target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "            y[i] = target_sequence\n",
    "            \n",
    "    return X, y, VOCAB_SIZE, ix_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función B\n",
    "Generación de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for generating text\n",
    "def generate_text(model, length, vocab_size, ix_to_char):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función C\n",
    "Obtener el tamaño del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_size(data_dir):\n",
    "    #Carga del archivo\n",
    "    data = open(data_dir, 'r').read()\n",
    "    #Caracteres unicos\n",
    "    chars = list(set(data))\n",
    "    return len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Entrenamiento y Prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de Diccionario\n",
    "\n",
    "*** ADVERTENCIA: NO EJECUTAR ESTA SECCIÓN SI ES QUE YA EXISTEN CHECKPOINTS Y IX_TO_CHAR ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uso de la Función A: carga de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 3196232 characters\n",
      "Vocabulary size: 86 characters\n",
      "['L', 'm', 'G', 'x', 'b', 'T', '9', '(', '8', 'R', 'M', 'n', ' ', 'N', 'a', 'v', ')', '!', 'g', 'H', 'K', 'l', 'c', 'k', 'r', 'X', '¤', 'q', '5', 'y', 'w', '*', 'A', ';', 'z', 'h', 'D', 'E', '»', 'u', '1', 'ï', 't', 'd', 'U', '6', '-', '¿', 'p', '3', 'O', 'Z', 'V', 'i', 'Q', '?', '.', 'P', 'J', 'W', 'o', '/', 'f', '7', '\"', '=', '\\n', '©', '0', ':', \"'\", 'I', ',', 'C', '\\xa0', 'Y', 's', 'ª', 'e', 'j', '2', 'F', 'Ã', 'B', 'S', '4']\n",
      "Number of sequences: 63924\n"
     ]
    }
   ],
   "source": [
    "# Creating training data\n",
    "X, y, VOCAB_SIZE, ix_to_char = load_data(DATA_DIR, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante guardar el diccionario `ix_to_char` en un archivo binario. Este debe ser cargado cada vez que se quiera retomar el entrenamiento o generar texto a partir de un *checkpoint*, debido a que el orden de los caracteres en el diccionario podría modificarse (no es un orden fijo).\n",
    "\n",
    "***NO MODIFICAR ESTE PICKLE AL REINICIAR EL NOTEBOOK PARA PROBAR CHECKPOINTS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No modificar el pickle al reiniciar el cuaderno de trabajo para probar checkpoints previos\n",
    "with open('ix_to_char.pickle', 'wb') as handle:\n",
    "    pickle.dump(ix_to_char, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'L', 1: 'm', 2: 'G', 3: 'x', 4: 'b', 5: 'T', 6: '9', 7: '(', 8: '8', 9: 'R', 10: 'M', 11: 'n', 12: ' ', 13: 'N', 14: 'a', 15: 'v', 16: ')', 17: '!', 18: 'g', 19: 'H', 20: 'K', 21: 'l', 22: 'c', 23: 'k', 24: 'r', 25: 'X', 26: '¤', 27: 'q', 28: '5', 29: 'y', 30: 'w', 31: '*', 32: 'A', 33: ';', 34: 'z', 35: 'h', 36: 'D', 37: 'E', 38: '»', 39: 'u', 40: '1', 41: 'ï', 42: 't', 43: 'd', 44: 'U', 45: '6', 46: '-', 47: '¿', 48: 'p', 49: '3', 50: 'O', 51: 'Z', 52: 'V', 53: 'i', 54: 'Q', 55: '?', 56: '.', 57: 'P', 58: 'J', 59: 'W', 60: 'o', 61: '/', 62: 'f', 63: '7', 64: '\"', 65: '=', 66: '\\n', 67: '©', 68: '0', 69: ':', 70: \"'\", 71: 'I', 72: ',', 73: 'C', 74: '\\xa0', 75: 'Y', 76: 's', 77: 'ª', 78: 'e', 79: 'j', 80: '2', 81: 'F', 82: 'Ã', 83: 'B', 84: 'S', 85: '4'}\n"
     ]
    }
   ],
   "source": [
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63924, 50, 86) (63924, 50, 86) 86\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "*** ADVERTENCIA: EJECUTAR DESDE ACÁ PARA ENTRENAR AL MODELO ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de la RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = vocab_size(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and compiling the Network\n",
    "model = Sequential()\n",
    "\n",
    "#Añadiendo las capas LSTM\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "#Añadiendo la operacion de salida\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#\"Compilando\" = instanciando la RNN con su función de pérdida y optimización\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?uyX0r0;;;;kkkkkkkkªªkkªkªRkRkRXXkiiiooï....GGGGGGYccccppppppppppP      )    ïïOOOOO8888(((gg(((gg(((gg(((gg¿¿¿¿¿¿¿ÃÃÃÃÃÃÃpppppppppppp     )     OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOcc"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?uyX0r0;;;;kkkkkkkkªªkkªkªRkRkRXXkiiiooï....GGGGGGYccccppppppppppP      )    ïïOOOOO8888(((gg(((gg(((gg(((gg¿¿¿¿¿¿¿ÃÃÃÃÃÃÃpppppppppppp     )     OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccccccpppppp)))         OOOOOccc'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate some sample before training to know how bad it is!\n",
    "generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan los pesos (y el diccionario de los one-hot-vectors) en caso haya habido un entrenamiento previo. WEIGHTS debe tener el valor del nombre del archivo de \"checkpoint\" guardado.\n",
    "\n",
    "Por ejemplo: ```WEIGHTS = \"checkpoint_layer_2_hidden_250_epoch_60.hdf5\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan los pesos de un entrenamiento previo (si se desea restaurar una ejecucion)\n",
    "#Se calcula el numero de epocas en base al nombre del archivo\n",
    "#Se carga el diccionario de caracteres (one-hot-vectors) para la generacion\n",
    "\n",
    "WEIGHTS = \"checkpoint_layer_2_hidden_250_epoch_10.hdf5\" \n",
    "\n",
    "if not WEIGHTS == '':\n",
    "    model.load_weights(WEIGHTS)\n",
    "    nb_epoch = int(WEIGHTS[WEIGHTS.rfind('_') + 1:WEIGHTS.find('.')])\n",
    "    with open('ix_to_char.pickle', 'rb') as handle:\n",
    "        ix_to_char = pickle.load(handle)\n",
    "else:\n",
    "    #Si se va a empezar de 0:\n",
    "    nb_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 10\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 499s 8ms/step - loss: 1.1963\n",
      "y the same time the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the sound of the\n",
      "\n",
      "Epoch: 11\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 512s 8ms/step - loss: 1.1852\n",
      "f the streets and the same time and the same time he was always strong and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength and strength \n",
      "\n",
      "Epoch: 12\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 489s 8ms/step - loss: 1.1755\n",
      "ut the soldiers and the same time the soldiers were being sent to his head and went to the countess and the same time the soldiers and the same time the soldiers were being sent to his head and went to the countess and the same time the soldiers and the same time the soldiers were being sent to his head and went to the countess and the same time the soldiers and the same time the soldiers were being sent to his head and went to the countess and the same time the soldiers and the same time the so\n",
      "\n",
      "Epoch: 13\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 473s 7ms/step - loss: 1.1665\n",
      "One can she want to see her and the countess who had been sent to the commander of the staff officer who had been sent to the commander of the staff officer who had been sent to the commander of the staff officer who had been sent to the commander of the staff officer who had been sent to the commander of the staff officer who had been sent to the commander of the staff officer who had been sent to the commander of the staff officer who had been sent to the commander of the staff officer who had\n",
      "\n",
      "Epoch: 14\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 507s 8ms/step - loss: 1.1585\n",
      "been to the countess and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the same time and the sam\n",
      "\n",
      "Epoch: 15\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 477s 7ms/step - loss: 1.1512\n",
      "n the third company and the count was still and then to the commander-in-chief was still the same thing and the count was still and then to the commander-in-chief was still the same thing and the count was still and then to the commander-in-chief was still the same thing and the count was still and then to the commander-in-chief was still the same thing and the count was still and then to the commander-in-chief was still the same thing and the count was still and then to the commander-in-chief w\n",
      "\n",
      "Epoch: 16\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 500s 8ms/step - loss: 1.1442\n",
      "just the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and th\n",
      "\n",
      "Epoch: 17\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 493s 8ms/step - loss: 1.1375\n",
      "justice of the staff officer who was still looking at him and said that he was about to do so. And the same thing he had not seen at the same time and said that the countess was sitting at the same time and said that the countess was sitting at the same time and said that the countess was sitting at the same time and said that the countess was sitting at the same time and said that the countess was sitting at the same time and said that the countess was sitting at the same time and said that the\n",
      "\n",
      "Epoch: 18\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 505s 8ms/step - loss: 1.1311\n",
      "; the same time and the same time and the same time and saw the man who had been as if they were all the same thing to do so. It was a long time for the first time the count was so far as the same time and said that the more the count was so far as the more the man who was saying to him and the same time and saw the man who had been as if they were all the same thing to do so. It was a long time for the first time the count was so far as the same time and said that the more the count was so far \n",
      "\n",
      "Epoch: 19\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 488s 8ms/step - loss: 1.1252\n",
      ". The old countess was a strange face and the same time and the same time and the same time she had not seen and the countess' house and the same time silently and straight into the drawing room to and the countess and the same time and the same time and the same time she had not seen and the countess' house and the same time silently and straight into the drawing room to and the countess and the same time and the same time and the same time she had not seen and the countess' house and the same \n",
      "\n",
      "Epoch: 20\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 495s 8ms/step - loss: 1.1198\n",
      "joined in the same way that he was always should be so strong that the soldiers and the same time he had been so strange and saw the soldiers and the same time he had been so strange and saw the soldiers and the same time he had been so strange and saw the soldiers and the same time he had been so strange and saw the soldiers and the same time he had been so strange and saw the soldiers and the same time he had been so strange and saw the soldiers and the same time he had been so strange and saw\n",
      "\n",
      "Epoch: 21\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 479s 7ms/step - loss: 1.1145\n",
      "Kutuzov to the sight of the staff officers and the same time and the same time and the staff officer who had been the same time and the same time and the staff officer who had been the same time and the same time and the staff officer who had been the same time and the same time and the staff officer who had been the same time and the same time and the staff officer who had been the same time and the same time and the staff officer who had been the same time and the same time and the staff offic\n",
      "\n",
      "Epoch: 22\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 502s 8ms/step - loss: 1.1095\n",
      "» the countess had been so clear to him and the countess was sitting at the countess that had been sent to his son whom he had been at the same time and the countess was sitting at the countess that had been sent to his son whom he had been at the same time and the countess was sitting at the countess that had been sent to his son whom he had been at the same time and the countess was sitting at the countess that had been sent to his son whom he had been at the same time and the countess was sit\n",
      "\n",
      "Epoch: 23\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 494s 8ms/step - loss: 1.1046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 the countess was sitting in the same way to the countess. He was already and was sitting in the same way to the countess. The count said the countess and the countess and the countess and the same thing that he was already been sent for him to be a man who had seemed to be a man who had seemed to be a man who had seemed to be a man who had seemed to be a man who had seemed to be a man who had seemed to be a man who had seemed to be a man who had seemed to be a man who had seemed to be a man wh\n",
      "\n",
      "Epoch: 24\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 505s 8ms/step - loss: 1.0999\n",
      "And the countess was sitting on the staff officer and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the countess and the \n",
      "\n",
      "Epoch: 25\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 485s 8ms/step - loss: 1.0955\n",
      "Go away! And what about your mother's health!\" said the countess.\n",
      "\n",
      "\"What a strange man was the matter with the princess and the countess' house and the countess was all the same things and the same time and the same thing as if in a state of the countess and the same thing as if in a state of the countess and the same thing as if in a state of the countess and the same thing as if she had not seen her for the first time to the countess and the same thing as if she had not seen her for the first \n",
      "\n",
      "Epoch: 26\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 493s 8ms/step - loss: 1.0914\n",
      "ment of the commander-in-chief was a strange and strength and the conversation with a smile to the commander in chief and the conversation with a smile to the commander in chief and the conversation with a smile to the commander in chief and the conversation with a smile to the commander in chief and the conversation with a smile to the commander in chief and the conversation with a smile to the commander in chief and the conversation with a smile to the commander in chief and the conversation w\n",
      "\n",
      "Epoch: 27\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 481s 8ms/step - loss: 1.0873\n",
      "She was a strange and a short study as the son of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the so\n",
      "\n",
      "Epoch: 28\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 506s 8ms/step - loss: 1.0834\n",
      "the countess was as if they were all that had been the same to him. He was as if they were all that had been the same to him. He was as if they were all that had been the same to him. He was as if they were all that had been the same to him. He was as if they were all that had been the same to him. He was as if they were all that had been the same to him. He was as if they were all that had been the same to him. He was as if they were all that had been the same to him. He was as if they were all\n",
      "\n",
      "Epoch: 29\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 480s 8ms/step - loss: 1.0797\n",
      "= the same time the soldiers of the count's house and the same thing to do so.\n",
      "\n",
      "\"You are to be a man who do not know the whole body to the peasants who has been to the count's house. The count was a strange face and the same thing to do so.\n",
      "\n",
      "\"You are to be a man who do not know the whole body to the peasants who has been to the count's house. The count was a strange face and the same thing to do so.\n",
      "\n",
      "\"You are to be a man who do not know the whole body to the peasants who has been to the count's \n",
      "\n",
      "Epoch: 30\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 485s 8ms/step - loss: 1.0761\n",
      "She was sitting in the same way the conversation with a smile.\n",
      "\n",
      "\"Why do you say that the countess was so much to be a man who was not a soldier with a smile.\n",
      "\n",
      "\"Why do you say that the countess was so much to be a man who was not a soldier with a smile.\n",
      "\n",
      "\"Why do you say that the countess was so much to be a man who was not a soldier with a smile.\n",
      "\n",
      "\"Why do you say that the countess was so much to be a man who was not a soldier with a smile.\n",
      "\n",
      "\"Why do you say that the countess was so much to be a ma\n",
      "\n",
      "Epoch: 31\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 492s 8ms/step - loss: 1.0726\n",
      "ked at her and her beautiful eyes glittering as if he were sent to the sound of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the soldiers of the \n",
      "\n",
      "Epoch: 32\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 495s 8ms/step - loss: 1.0694\n",
      "Uncle's\" return to the commander of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the streets and the same time the soldiers of the \n",
      "\n",
      "Epoch: 33\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 505s 8ms/step - loss: 1.0661\n",
      "join in the distance of the door and the sound of the country of the contrary, the sound of the country of the commander-in-chief was the same thing was conscious of the conversation in the same way that the sole of the soldiers of the conversation in the same way that the sole of the soldiers of the conversation in the same way that the sole of the soldiers of the conversation in the same way that the sole of the soldiers of the conversation in the same way that the sole of the soldiers of the \n",
      "\n",
      "Epoch: 34\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 486s 8ms/step - loss: 1.0632\n",
      "ze the countess, and the count was standing on the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff officer of the staff of\n",
      "\n",
      "Epoch: 35\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 472s 7ms/step - loss: 1.0602\n",
      "?\" asked Pierre.\n",
      "\n",
      "\"What is the matter with your honor!\" and the countess was so strong and was still the same time the countess and the same streets and the same street of the suite and the street was standing by the streets to the sound of the commander-in-chief was still stretched his shoulders and stretched his shoulders and stretched his shoulders and stretched his shoulders and stretched his shoulders and stretched his shoulders and stretched his shoulders and stretched his shoulders and st\n",
      "\n",
      "Epoch: 36\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 486s 8ms/step - loss: 1.0571\n",
      "Natasha seemed to him a moment. \"I was all right, and there is no more intimate friends, and the soldiers in the same way that it is not a single man.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I don't know what you would not be all the same thing. I won't go to the countess' door was a man who was not at all and did not like to see him and the same thing is the only thing of the countess and the count with a smile of destroy the country house on the strength of the contrary, the count was a man who was not at all and did not like to \n",
      "\n",
      "Epoch: 37\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 470s 7ms/step - loss: 1.0547\n",
      "ing the street was standing by the stalls and the soldiers who were sitting and shouting and shouting and shook his head and said to him and said to him.\n",
      "\n",
      "\"What do you think of the day of the things and the state of things and the state of this interests of the country and the soldiers who were sitting on the staff of the commander in chief and the same time the soldiers who were sitting in the same way the soldiers who were sitting and shouting and shouting and shouting and shook his head and s\n",
      "\n",
      "Epoch: 38\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 476s 7ms/step - loss: 1.0520\n",
      "7 the soldiers started and the sound of a strange and the same thing the princess and the sound of the suite who had been the same thing the fate of the princess and the sound of the suite who had been the same thing the fate of the princess and the sound of the suite who had been the same thing the fate of the princess and the sound of the suite who had been the same thing the fate of the princess and the sound of the suite who had been the same thing the fate of the princess and the sound of t\n",
      "\n",
      "Epoch: 39\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 478s 7ms/step - loss: 1.0494\n",
      "Andrew with a smile of the street. \"What a splendid reign of the countess' house with a smile of the soldiers of the country house on the staff officer who was saying to him and then all the same things the countess in the same way the countess and the same strength and the staff officer who was saying to him and then all the same things the countess in the same way the countess and the same strength and the staff officer who was saying to him and then all the same things the countess in the sam\n",
      "\n",
      "Epoch: 40\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 473s 7ms/step - loss: 1.0470\n",
      "Yes, yes, I know that the man who says it is all very well?\" thought Prince Andrew, \"and that it was not a soldier with a sigh of the contrary it was a man of life and the same to me that the man who had stayed in the same way that the soldiers of the streets to the soldiers and the same thing as the strange and stern but the same time and said that the commander-in-chief was as conditions of the campaign of 1812 for the first time the sound of the soldiers and the same time as if the same time \n",
      "\n",
      "Epoch: 41\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 476s 7ms/step - loss: 1.0445\n",
      "The count had been sent to the count and his son whom he had not seen them to say that the count was not seen in the forest and still less any of them and the count was a great deal were all the same feelings which had not yet as commander-in-chief was already been sent to the count and his son whom he had not seen them to say that the count was not seen in the forest and still less any of them and the count was a great deal were all the same feelings which had not yet as commander-in-chief was \n",
      "\n",
      "Epoch: 42\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 509s 8ms/step - loss: 1.0423\n",
      "Count Rostov had been told that the count had been told that the count had been told that the count had been told that the count had been told that the count had been told that the count had been told that the count had been told that the count had been told that the count had been told that the count had been told that the count had been told that the count had been told that the count had been told that the count had been told that the count had been told that the count had been told that the \n",
      "\n",
      "Epoch: 43\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 469s 7ms/step - loss: 1.0400\n",
      "'s attention to the devil to be an order to adore and the state of the commander-in-chief was standing at the same time and drove up to the door and the stranger and strangely and sternly and stepping over the day before him with his hand and saw that the countess had been sent to the soldiers who were all that he was saying to him and had said that the countess had been sent to the soldiers who were all that he was saying to him and had said that the countess had been sent to the soldiers who w\n",
      "\n",
      "Epoch: 44\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 466s 7ms/step - loss: 1.0380\n",
      "; but he was a stir among the movement of the Russian army was already serving in her hands.\n",
      "\n",
      "\"Well, good-by, Countess Mary and the countess and the same thing was already been sent to the countess and the same thing as the staff officer on the first man who was always the same time and said that he was a stir as the countess and the same thing as the staff officer on the first man who was always the same time and said that he was a stir as the countess and the same thing as the staff officer on\n",
      "\n",
      "Epoch: 45\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 469s 7ms/step - loss: 1.0358\n",
      "s and the consequence of the conception of the contrary, the countess was stronger that he was strange to him that he was saying to her husband and the count and the strength of the contrary, the conversation was described to him and the count and the strength of the contrary, the conversation was described to him and the count and the strength of the contrary, the conversation was described to him and the count and the strength of the contrary, the conversation was described to him and the coun\n",
      "\n",
      "Epoch: 46\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 477s 7ms/step - loss: 1.0338\n",
      "1807, were standing before him and the same street they were all as if he were at the same time and the same street with his head and the same spring of the contrary, the sound of the princesses was a significant and sat down by the sofa. Only the men were standing before him and the same street they were all as if he were at the same time and the same street with his head and the same spring of the contrary, the sound of the princesses was a significant and sat down by the sofa. Only the men we\n",
      "\n",
      "Epoch: 47\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 469s 7ms/step - loss: 1.0321\n",
      "4Vjet, and the Emperor was the same thing and see the man who had been sent to the country house on the staff of the commander-in-chief of the commander-in-chief was the same thing as the son is the same thing as the strength of the commander-in-chief of the commander-in-chief was the same thing as the son is the same thing as the strength of the commander-in-chief of the commander-in-chief was the same thing as the son is the same thing as the strength of the commander-in-chief of the commander\n",
      "\n",
      "Epoch: 48\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 462s 7ms/step - loss: 1.0302\n",
      "and the countess was a stout man with a smile that he was always strong and that he was always saying:\n",
      "\n",
      "\"I have not seen them to see him and that is the matter with you? What does it matter to the countess, when I was not a single brown and the countess,\" said the countess.\n",
      "\n",
      "\"Why are you staying in the morning of the Emperor Alexander,\" said the countess.\n",
      "\n",
      "\"Why are you staying in the morning of the Emperor Alexander,\" said the countess.\n",
      "\n",
      "\"Why are you staying in the morning of the Emperor Alexand\n",
      "\n",
      "Epoch: 49\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 463s 7ms/step - loss: 1.0282\n",
      "!\" said the countess, with a smile of despair, sat down on the sofa and then and was still stood and all the same things in the same way. The old prince had been received and then and when he had been to be the only person to the sound of the countess and the countess and the countess and the countess and the countess and the countess and the countess and then the countess and the countess and the countess and the countess and then the countess and the countess and the countess and then to the d\n",
      "\n",
      "Epoch: 50\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63924/63924 [==============================] - 468s 7ms/step - loss: 1.0267\n",
      "e the count was a man of genius to be an order to see him and the count to see him and the count to see him and so on. The count suddenly finds most delightful and confusion and regard to his son had been sent to the count and his son had been sent to the count and his son had been sent to the count and his son had been sent to the count and his son had been sent to the count and his son had been sent to the count and his son had been sent to the count and his son had been sent to the count and \n",
      "\n",
      "Epoch: 51\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 463s 7ms/step - loss: 1.0250\n",
      "xpression whether he was so stronger and serious and state of the commander-in-chief's staff and the count was a stranger and stores, and the count was a straight thrown on the sofa. He was a straight there was a stout position where the count had been sent to the count, and the count was a proceded and strength of his house the sound of the country house on the streets of the soldiers that he was asked about the prince's division was the count and the countess was still at the same time and the\n",
      "\n",
      "Epoch: 52\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 470s 7ms/step - loss: 1.0233\n",
      "One can do that said the old man who was already sinning the conversation with the same smile.\n",
      "\n",
      "\"Where are you going? I know you wish to see him and the count was a second and artillery officers and the count and men who were all the same thing to do and have a pleasure of it!\" he shouted and said:\n",
      "\n",
      "\"I don't understand the words to the Emperor.\"\n",
      "\n",
      "\"I know you are not at all as the fact that the old man is not a single month. I was there in the morning, but the count was a serious to speak to him \n",
      "\n",
      "Epoch: 53\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 460s 7ms/step - loss: 1.0218\n",
      "e of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of the commander of th\n",
      "\n",
      "Epoch: 54\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 459s 7ms/step - loss: 1.0201\n",
      "= Julie's men and the expression of a man who was still sitting at the door and the study to the drawing room with his hand to his heart and asked him to say that he had not seen him and the same stern springling and something to the countess) who had been an army of the country were at the same time and the study and the staff of the house and at the same time and the study to the drawing room with his hand to his heart and asked him to say that he had not seen him and the same stern springling\n",
      "\n",
      "Epoch: 55\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 460s 7ms/step - loss: 1.0185\n",
      ": \"Yes, that is the matter?\" asked Pierre.\n",
      "\n",
      "\"I am sorry for your sake then, your excellency!\" said the countess, \"and he is a fine fellow, I shall never be a battle and there was no one and the countess, your excellency,\" said the countess, \"and he is a fine fellow, I shall never be a battle and there was no one and the countess, your excellency,\" said the countess, \"and he is a fine fellow, I shall never be a battle and there was no one and the countess, your excellency,\" said the countess, \"an\n",
      "\n",
      "Epoch: 56\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 460s 7ms/step - loss: 1.0172\n",
      "¿justify the tramp of the commander-in-chief of the commander-in-chief's staff, and the same face was disconcerted. As soon as the soldiers sat up and down the room.\n",
      "\n",
      "\"I said the one he went on!\" he shouted to the door and the sound of the commander-in-chief (the soldiers saw the door of the room.\n",
      "\n",
      "\"I saw the count has been a moment. I can't do it at all at once and the countess.\"\n",
      "\n",
      "\"Oh, how did you get the father who has been the same thing is the cause of the commander-in-chief's suite. A dance\n",
      "\n",
      "Epoch: 57\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 458s 7ms/step - loss: 1.0159\n",
      "Lazarev, who had been sent to the countess' rooms, when he had been able to go away from the commissariat departments and the same thing as the stranger sat in the same way and the same thing as the stranger sat in the same way and the same thing as the stranger sat in the same way and the same thing as the stranger sat in the same way and the same thing as the stranger sat in the same way and the same thing as the stranger sat in the same way and the same thing as the stranger sat in the same w\n",
      "\n",
      "Epoch: 58\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 459s 7ms/step - loss: 1.0145\n",
      "Uncle's\" longer and moved and stretched his legs and the same thing as the soldiers of the conversation and was standing on the staff of the committeemen only the same thing and the same thing to her husband and especially that he was already attending to him that he was as if the soldiers of the conversation and was standing on the staff of the committeemen only the same thing and the same thing to her husband and especially that he was already attending to him that he was as if the soldiers of\n",
      "\n",
      "Epoch: 59\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 459s 7ms/step - loss: 1.0131\n",
      "ke a man who was still the same at the same time she had not seen him since the same time he had seen and a small bono brought back to his study. He was still the same at the same time she had not seen him since the same time he had seen and a small bono brought back to his study. He was still the same at the same time she had not seen him since the same time he had seen and a small bono brought back to his study. He was still the same at the same time she had not seen him since the same time he\n",
      "\n",
      "Epoch: 60\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 460s 7ms/step - loss: 1.0118\n",
      "ªte at the same time and the soldiers of the country in the same way that the countess in the same way that the countess in the same way that the countess in the same way that the countess in the same way that the countess in the same way that the countess in the same way that the countess in the same way that the countess in the same way that the countess in the same way that the countess in the same way that the countess in the same way that the countess in the same way that the countess in th\n",
      "\n",
      "Epoch: 61\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 460s 7ms/step - loss: 1.0106\n",
      "On the contrary, the count was a good thing that had been sent to the count and moved away from the table. He was sitting in the drawing room. The old prince was silent and then to say to him and asked him to say to him and was still the same things about Kutuzov, and the count was at the same time and the count was a good thing and the count and his son to himself, and the same thing he had been so good as to a man who had been sent to the count and moved away from the table. He was sitting in \n",
      "\n",
      "Epoch: 62\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 459s 7ms/step - loss: 1.0094\n",
      "/y he had not been able to reach the strange conflicts of the commander-in-chief in the same way, the strange consciousness of the commander-in-chief is already behind the countess. The countess was sitting at the commander-in-chief was to be seen in the same position of the commander-in-chief in the carts that had been so clear and conscious of the commander-in-chief is already becoming in the country de Tolly, which he had seen and at the same time for the first time the strange and simple and\n",
      "\n",
      "Epoch: 63\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 460s 7ms/step - loss: 1.0085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me to the commander-in-chief. \"If you want to come out at the countess' relations with the princess and the count to remain in the morning and the count and his wife who had been sent to the count and had to be of the conversation with the same thing as he had been an order to remain with him to see him and the count was as if she had not the commander of the staff of the commander of the room with a smile that he would be able to remember the countess and the count to her father and mother, and\n",
      "\n",
      "Epoch: 64\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 456s 7ms/step - loss: 1.0074\n",
      "; but he did not see the countess was so delighted to see him and the same as the same thing and the same as the Emperor and the countess was so fasting and seeming not to be a passionate and ascending the princesses, and the count was at the same time and did not see him and did not answer the conversation and was standing by the same way that she was not at all asked: the old prince had been preparing to reply to the count and mouth to his face and anger and said something in the same way that\n",
      "\n",
      "Epoch: 65\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 457s 7ms/step - loss: 1.0064\n",
      "ïjeak the field of battle and the strange conversation with a smile of defending the manifestations of the collar that at the same time the countess in the face of the street.\n",
      "\n",
      "The countess in the same way the laws of the commander in chief. The old prince had a silent while the sick man was saying that the commander of the commanders and the strangelent for a long time he was about to be an unpleasant town. The countess went to the door of the street. The countess in the face of the princess an\n",
      "\n",
      "Epoch: 66\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 456s 7ms/step - loss: 1.0054\n",
      "re the count and experienced at the commander of the campfires, and the same thing and the same thing and the same thing and she would have said this and the consequent campaign with the same smile on his face and said it was a man of late at the same time and said something to him. He was sitting at the staff officer in the morning as to what he had said and seemed to him that the commander of the countess sent to the conception of the campaign that the count was a good thing and the same thing\n",
      "\n",
      "Epoch: 67\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 457s 7ms/step - loss: 1.0042\n",
      ". The old prince was at the same time and a stout man who had come to the count and the street was about to contradict themselves to him that the count was the same thing and the street was already and all the same things to the state of the campaign of 1812 and the conversation was about to get away as quickly as possible. The count started the count and laughter to him, and then the count was a stranger and a state of things there was a stout man who stood before him to reply to the count and \n",
      "\n",
      "Epoch: 68\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 455s 7ms/step - loss: 1.0031\n",
      "9 (the countess was a stir as the count was as impossible for the first time the streets to get away as quickly as possible. The countess went on to the count's anxious and sat down on a sofa in his soul. He felt that the count was a stranger, and the same state of the commander-in-chief's staff, and the same thing to do so.\n",
      "\n",
      "\"You know they are all my eyes, and the count was as impossible:\n",
      "\n",
      "\"Why did you believe in the manifestations of the truth.\"\n",
      "\n",
      "\"Oh, don't see it and I am so simplicited! You \n",
      "\n",
      "Epoch: 69\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 456s 7ms/step - loss: 1.0023\n",
      "When they were all the same this the man who says that this was the cause of the people and the countess was so plain that this was the cause of the people and the countess was so plain that this was the cause of the people and the countess was so plain that this was the cause of the people and the countess was so plain that this was the cause of the people and the countess was so plain that this was the cause of the people and the countess was so plain that this was the cause of the people and \n",
      "\n",
      "Epoch: 70\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 457s 7ms/step - loss: 1.0015\n",
      "!\" he said, and gazed at her sisters, and then to him that the commanders of command, and the count was reading the line of command to the country and the same thing as the stranger sat down at the countess' eyes, and the count and Sonya and the count was reading the line of command, and the count was reading the line of command to the country and the same thing as the stranger sat down at the countess' eyes, and the count and Sonya and the count was reading the line of command, and the count wa\n",
      "\n",
      "Epoch: 71\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 457s 7ms/step - loss: 1.0006\n",
      "© Cossack who had been taken on the old prince), \"but the count was a strange to speak to him and that the commander of the commissariat of a perfection of the commander-in-chief was to be sent to the conversation to the other way for the first time the soldiers of the commissariat of a perfection of the committee Princess Mary was already strained to the sound of the commander in chief and the contemptuous smiles of the campaign of 1812 the thought that he was asked to be a man of life and the \n",
      "\n",
      "Epoch: 72\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 456s 7ms/step - loss: 0.9996\n",
      "quietly and took his hand to the soldiers and without any reason to find the form of his soul and the count with a smile of despairing--as if she had seen as he was still stronger than a man who was sitting in the same place and the sound of the principal expression of his dissatisfaction with the same smile on his lips and shook his head and said:\n",
      "\n",
      "\"Andrew, what a divine instance of the principal Army of Weyrother, and I shall not have been the same thing!\" said the countess.\n",
      "\n",
      "\"Yes, yes, of cou\n",
      "\n",
      "Epoch: 73\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 457s 7ms/step - loss: 0.9987\n",
      "the commander of the staff of the commanders and the sound of the death of Count Bezukhov had to have a footman who had been sent to the commander of the staff. The moment of the commander of the story of the commanders of the commanders and the countess and the countess and the steward met at the staff of the army which he had seen and the same way than the old prince was as she stood in a tone of smoke. The soldiers were stronger than the commanders and shouted \"Hurrah!\" and she seemed to be a\n",
      "\n",
      "Epoch: 74\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 456s 7ms/step - loss: 0.9978\n",
      "9 (the countess was the same thing. He was a stranger and straight there and the constant which was a stick of the camp, the soldiers who were saying to the countess, and the count was a man of honor and the people who had been sent to the conclusion that the French were still more than any of them the commander of the contradiction, and the man who had come to the conclusion that had been sent to the conclusion that had been sent to the conclusion that the French were still more than any of the\n",
      "\n",
      "Epoch: 75\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 457s 7ms/step - loss: 0.9972\n",
      "  had to be done at the count and the countess' health, and the countess was a stout position of the movement of the movement of the people of the battle of Borodino and the country were the cause of the people of September, the countess was so full on the contrary to the first contrary, the countess and the countess, who had been sent to the porch. It was a man who was sitting in the same way that the countess was a stout position in the same way that he was asked where he was still sitting abo\n",
      "\n",
      "Epoch: 76\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 457s 7ms/step - loss: 0.9964\n",
      "6 On the third day all the same the men were sitting on a sofa before her brother and the count was as if she had been so strange to be seen in the middle of the room.\n",
      "\n",
      "\"It would be a girls and my daughter's death of the count's all of you all about it all the same to me?\" he asked them to the count, and then to him sat down again as she had a stout man with a smile of life in the middle of the room.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"It's not the commander-in-chief?\" he said, and said to him: \"I love you more than any of you, \n",
      "\n",
      "Epoch: 77\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 457s 7ms/step - loss: 0.9956\n",
      "3) Sunday and said something to him and was about to say to him and had been to be seen. But when he had to do with the countess, and the count and the same sport she had said and went out of the room.\n",
      "\n",
      "\"Mamma, don't see that this is a moment of my brother with you? It is not to love your last part of the commissary activity of the campaign and the countess, and that if he had not been able to do anything of the commander-in-chief. He was silent and said something to him and was about to say to \n",
      "\n",
      "Epoch: 78\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 456s 7ms/step - loss: 0.9949\n",
      "What was the same to me that I should be a child?\" said the countess, \"but I do not know the count and the same as to the Governor's,\" he added, turning to his study. \"And who is it of me? She is the same as to the Governor's,\" he added, turning to his study. \"And who is it of me? She is the same as to the Governor's,\" he added, turning to his study. \"And who is it of me? She is the same as to the Governor's,\" he added, turning to his study. \"And who is it of me? She is the same as to the Govern\n",
      "\n",
      "Epoch: 79\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 459s 7ms/step - loss: 0.9941\n",
      "ke a man who says that the commander-in-chief) to the left flank of the ballroom and the strength of his head and the sound of a series of victories and the shouting and the shouting of the country house when he had a long time (it was a man of late at the beginning of the campaign that had been sent to the possibility of his own accordaze and the countess and the streets of his face and the shouts of the committees of the commander-in-chief's soul. And the count was already at the beginning of "
     ]
    }
   ],
   "source": [
    "# Training if there is no trained weights specified\n",
    "\n",
    "#Esta es la iteración importante\n",
    "#Pueden cambiar la condición para que termine en un determinado numero de epochs.\n",
    "while True:\n",
    "    print('\\n\\nEpoch: {}\\n'.format(nb_epoch))\n",
    "    #Ajuste del modelo, y entrenamiento de 1 epoca\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=1)\n",
    "    nb_epoch += 1\n",
    "    #Generacion de un texto al final de la epoca\n",
    "    generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "    #Pueden modificar esto para tener más checkpoints\n",
    "    if nb_epoch % 10 == 0:\n",
    "        model.save_weights('checkpoint_layer_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch))\n",
    "    if nb_epoch == 80:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generación de texto\n",
    "\n",
    "Si instancian el modelo y sus parametros (ejecutando algunas celdas preliminares), y tienen los 2 archivos requeridos (.pickle y .hdf5) pueden generar el texto.\n",
    "Si usted va a cargar defrente un *checkpoint*, ejecutar los siguientes 2 módulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = vocab_size(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and compiling the Network\n",
    "model = Sequential()\n",
    "\n",
    "#Añadiendo las capas LSTM\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "#Añadiendo la operacion de salida\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#\"Compilando\" = instanciando la RNN con su función de pérdida y optimización\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generar texto dependiendo del WEIGHTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rostov was a strange to see her at the same time and the countess asked her to be seen at the commander-in-chief. \"I have been told that the prince said to him about the matter. I will come to see him and that I have to see him and that I have to see him and that I have to see him and that I have to see him and that I have to see him and that I have to see him and that I have to see him and that I have to see him and that I have to see him and that I have to see him and that I have to see him an\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cuidar de no reemplazar el pickle original\n",
    "WEIGHTS = 'checkpoint_layer_2_hidden_250_epoch_80.hdf5'\n",
    "nb_epoch = int(WEIGHTS[WEIGHTS.rfind('_') + 1:WEIGHTS.find('.')])\n",
    "# Loading the trained weights\n",
    "model.load_weights(WEIGHTS)\n",
    "with open('ix_to_char.pickle', 'rb') as handle:\n",
    "    ix_to_char = pickle.load(handle)\n",
    "generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al presentarlo a sus JPs deben poder destacar y discutir lo siguiente como mínimo:\n",
    "- ¿En qué consiste el texto de entrada?\n",
    "- ¿Qué patrones de secuencias se han ido aprendiendo a lo largo de diferentes épocas? (palabras,  oraciones, párrafos, u otros de acuerdo al dominio de los textos)\n",
    "- ¿Hay consistencia en la generación? (discusión)\n",
    "\n",
    "Otros resultados complementarios (y opcionales) que pueden contribuir a su discusión de resultados son:\n",
    "- Variación de parámetros de la red neuronal.\n",
    "- Identificación de términos y o frases completamente nuevas que el modelo ha generado (es decir, que no se encuentran en el texto original)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
