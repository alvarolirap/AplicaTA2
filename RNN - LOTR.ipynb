{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN y Character-level Neural LM (LOTR)\n",
    "Basado en: https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/\n",
    "<br><br>El cuaderno de trabajo presenta una aplicación directa de un **Character-Level Neural Language Model** para generar automáticamente texto en base a un archivo/documento textual de entrada. En este caso, se usaron los libros de LOTR (The Lord of the Rings)\n",
    "\n",
    "<br>**Se recomienda generar un notebook nuevo, colocando sólo las instrucciones útiles para su prueba de generación de textos. Modifique y ordene lo que considere conveniente para una mayor legibilidad y comprensión de su prueba (para la revisión con los JPs en el Lab. 10)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación (_utils_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e94825e2c697>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSimpleRNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definición de argumentos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Archivo de texto \n",
    "DATA_DIR = \"./lotr.txt\" \n",
    "#Modificar BATCH_SIZE o HIDDEN_DIM en caso tengan problemas de memoria\n",
    "BATCH_SIZE = 50 \n",
    "HIDDEN_DIM = 250 #500\n",
    "#Parametro para longitud de secuencia a analizar\n",
    "SEQ_LENGTH = 50 \n",
    "#Parametro para cargar un pesos previamente entrenados (checkpoint)\n",
    "WEIGHTS = '' \n",
    "\n",
    "#Parametro para indicar cuantos caracteres generar en cada prueba\n",
    "GENERATE_LENGTH = 500 \n",
    "#Parametros para la red neuronal\n",
    "LAYER_NUM = 2 \n",
    "NB_EPOCH = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función A:\n",
    "<br>(1) Carga de un archivo de texto, (2) Construcción de estructuras de entrada y salida de la red**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for preparing the training data\n",
    "def load_data(data_dir, seq_length):\n",
    "    #Carga del archivo\n",
    "    data = open(data_dir, 'r').read()\n",
    "    #Caracteres unicos\n",
    "    chars = list(set(data))\n",
    "    VOCAB_SIZE = len(chars)\n",
    "\n",
    "    print('Data length: {} characters'.format(len(data)))\n",
    "    print('Vocabulary size: {} characters'.format(VOCAB_SIZE))\n",
    "    print(chars)\n",
    "    \n",
    "    #Indexacion de los caracteres\n",
    "    ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "    char_to_ix = {char:ix for ix, char in enumerate(chars)}\n",
    "    \n",
    "    #Estructuras de entrada y salida\n",
    "    NUMBER_OF_SEQ = int(len(data)/seq_length)\n",
    "    print('Number of sequences: {}'.format(NUMBER_OF_SEQ))\n",
    "    X = np.zeros((NUMBER_OF_SEQ, seq_length, VOCAB_SIZE))\n",
    "    y = np.zeros((NUMBER_OF_SEQ, seq_length, VOCAB_SIZE))\n",
    "    \n",
    "    for i in range(0, NUMBER_OF_SEQ):\n",
    "        #LLenado de la estructura de entrada X\n",
    "        X_sequence = data[i*seq_length:(i+1)*seq_length]\n",
    "        X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n",
    "        #one-hot-vector (input)\n",
    "        input_sequence = np.zeros((seq_length, VOCAB_SIZE))  \n",
    "        #uso del diccionario para completar el one-hot-vector\n",
    "        for j in range(seq_length):\n",
    "            input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "            X[i] = input_sequence\n",
    "            \n",
    "        #Llenado de la estructura de salida y\n",
    "        y_sequence = data[i*seq_length+1:(i+1)*seq_length+1]\n",
    "        y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
    "        #one-hot-vector (output)\n",
    "        target_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "        #uso del diccionario para completar el one-hot-vector\n",
    "        for j in range(seq_length):\n",
    "            target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "            y[i] = target_sequence\n",
    "            \n",
    "    return X, y, VOCAB_SIZE, ix_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función B:\n",
    "<br>Generación de textos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for generating text\n",
    "def generate_text(model, length, vocab_size, ix_to_char):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y Prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uso de la Función A: carga de los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 822197 characters\n",
      "Vocabulary size: 84 characters\n",
      "['Y', ',', '\\n', '3', 'O', '\\\\', 'x', 't', 'F', 'l', 'D', '_', 'V', 'd', 'E', 'j', 'L', 'T', '1', 'v', '\"', 'k', 'W', 'i', 'q', 'Q', 'C', 'B', 'A', 'R', 'g', '’', 'X', 'I', 'N', 'S', '.', '®', ')', '8', 'J', '7', 'm', '0', 'r', 'e', \"'\", 'u', '!', '4', ' ', '5', 'Z', 'c', ';', 'H', 'G', 'U', 'w', '“', 'p', '?', 'P', 's', '*', '—', '2', '/', 'h', '(', 'y', 'o', '9', ':', 'K', 'f', 'b', 'M', 'z', 'n', '6', '-', 'a', '^']\n",
      "Number of sequences: 16443\n"
     ]
    }
   ],
   "source": [
    "# Creating training data\n",
    "X, y, VOCAB_SIZE, ix_to_char = load_data(DATA_DIR, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Es importante guardar el diccionario `ix_to_char` en un archivo binario. Este debe ser cargado cada vez que se quiera retomar el entrenamiento o generar texto a partir de un checkpoint, debido a que el orden de los caracteres en el diccionario podría modificarse (no es un orden fijo)**\n",
    "<br>**NO MODIFICAR ESTE PICKLE AL REINICIAR EL NOTEBOOK PARA PROBAR CHECKPOINTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No modificar el pickle al reiniciar el cuaderno de trabajo para probar checkpoints previos\n",
    "with open('ix_to_char.pickle', 'wb') as handle:\n",
    "    pickle.dump(ix_to_char, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '’', 1: '\"', 2: 'y', 3: ')', 4: '0', 5: 'z', 6: 'G', 7: 'Q', 8: '3', 9: 'N', 10: '-', 11: '*', 12: ':', 13: ' ', 14: 'F', 15: 'c', 16: '1', 17: '6', 18: 'x', 19: 'R', 20: 'U', 21: 'n', 22: '5', 23: '(', 24: 'g', 25: 'm', 26: 'l', 27: 'v', 28: 'Z', 29: 'h', 30: 'A', 31: 'e', 32: 'W', 33: 'H', 34: 'i', 35: ';', 36: 'D', 37: 'P', 38: 'b', 39: 'o', 40: \"'\", 41: 's', 42: 'q', 43: 'Y', 44: '7', 45: '2', 46: 't', 47: 'O', 48: '“', 49: 'p', 50: '®', 51: 'J', 52: 'C', 53: 'r', 54: '9', 55: '^', 56: ',', 57: 'V', 58: 'd', 59: '_', 60: '.', 61: 'u', 62: '\\n', 63: '8', 64: 'S', 65: 'a', 66: 'T', 67: 'B', 68: 'K', 69: '?', 70: 'E', 71: 'w', 72: '!', 73: 'M', 74: 'k', 75: 'I', 76: 'j', 77: 'f', 78: '/', 79: '\\\\', 80: '—', 81: 'X', 82: '4', 83: 'L'}\n"
     ]
    }
   ],
   "source": [
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16443, 50, 84) (16443, 50, 84) 84\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creación de la RNN (LSTM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and compiling the Network\n",
    "model = Sequential()\n",
    "\n",
    "#Añadiendo las capas LSTM\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "#Añadiendo la operacion de salida\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#\"Compilando\" = instanciando la RNN con su función de pérdida y optimización\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prueba inicial de creación de 500 caracteres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAzzz,,,!!!!JJ))ZZZZpZpZZprrrr;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CAzzz,,,!!!!JJ))ZZZZpZpZZprrrr;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate some sample before training to know how bad it is!\n",
    "generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se cargan los pesos (y el diccionario de los one-hot-vectors) en caso haya habido un entrenamiento previo**\n",
    "<br>WEIGHTS debe tener el valor del nombre del archivo de \"checkpoint\" guardado. Por ejemplo:\n",
    "<br>```WEIGHTS = \"checkpoint_layer_2_hidden_250_epoch_60.hdf5\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan los pesos de un entrenamiento previo (si se desea restaurar una ejecucion)\n",
    "#Se calcula el numero de epocas en base al nombre del archivo\n",
    "#Se carga el diccionario de caracteres (one-hot-vectors) para la generacion\n",
    "if not WEIGHTS == '':\n",
    "    model.load_weights(WEIGHTS)\n",
    "    nb_epoch = int(WEIGHTS[WEIGHTS.rfind('_') + 1:WEIGHTS.find('.')])\n",
    "    with open('ix_to_char.pickle', 'rb') as handle:\n",
    "        ix_to_char = pickle.load(handle)\n",
    "else:\n",
    "    #Si se va a empezar de 0:\n",
    "    nb_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ENTRENAMIENTO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aoncevay/.conda/envs/chana/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 93s 6ms/step - loss: 2.4602\n",
      "1 the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the store and the stor\n",
      "\n",
      "Epoch: 1\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 94s 6ms/step - loss: 1.8765\n",
      "the stood and the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the st\n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 94s 6ms/step - loss: 1.6646\n",
      "The was a stranger of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great of the great \n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 94s 6ms/step - loss: 1.5435\n",
      "®re the mountains of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the man of the ma\n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 94s 6ms/step - loss: 1.4673\n",
      "Lead and strange the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the stairs of the sta\n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 93s 6ms/step - loss: 1.4139\n",
      ", and the hobbits had been so that he had not so for a moment to the horse of the Entwives of the horse of the Entwives of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of the horse of \n",
      "\n",
      "Epoch: 6\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 92s 6ms/step - loss: 1.3736\n",
      "y the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the ston\n",
      "\n",
      "Epoch: 7\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 92s 6ms/step - loss: 1.3399\n",
      "K he was the stars of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of th\n",
      "\n",
      "Epoch: 8\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 91s 6ms/step - loss: 1.3114\n",
      "X the wind of the woods of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great stones of the great sto\n",
      "\n",
      "Epoch: 9\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 93s 6ms/step - loss: 1.2864\n",
      "8 he said the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of \n",
      "\n",
      "Epoch: 10\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 91s 6ms/step - loss: 1.2636\n",
      ") and the light of the water and the light of the stream. The water of the Enemy in the dark horses of the trees. The water of the Enemy in the dark horses of the trees. The water of the Enemy in the dark horses of the trees. The water of the Enemy in the dark horses of the trees. The water of the Enemy in the dark horses of the trees. The water of the Enemy in the dark horses of the trees. The water of the Enemy in the dark horses of the trees. The water of the Enemy in the dark horses of the t\n",
      "\n",
      "Epoch: 11\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 90s 5ms/step - loss: 1.2427\n",
      "“ he saw that the stars of the stair was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a great stone was a grea"
     ]
    }
   ],
   "source": [
    "# Training if there is no trained weights specified\n",
    "\n",
    "#Esta es la iteración importante\n",
    "#Pueden cambiar la condición para que termine en un determinado numero de epochs.\n",
    "while True:\n",
    "    print('\\n\\nEpoch: {}\\n'.format(nb_epoch))\n",
    "    #Ajuste del modelo, y entrenamiento de 1 epoca\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=1)\n",
    "    nb_epoch += 1\n",
    "    #Generacion de un texto al final de la epoca\n",
    "    generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "    #Pueden modificar esto para tener más checkpoints\n",
    "    if nb_epoch % 10 == 0:\n",
    "        model.save_weights('checkpoint_layer_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRUEBA errónea de un checkpoint anterior**\n",
    "<br>Al reiniciar el notebook, y cargar un checkpoint sin cargar correctamente el diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "’.hO1®.dO)®OxnD.’i.6O8w.)Y . Y.hww. 6w.h Y)wh.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w.h HwO7.Yn. 6w\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "WEIGHTS = \"checkpoint_layer_2_hidden_250_epoch_10_run1.hdf5\"\n",
    "# Loading the trained weights\n",
    "model.load_weights(WEIGHTS)\n",
    "generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "print('\\n\\n')\n",
    "\n",
    "#El ejemplo impreso en realidad es usando un \"diccionario\" re-ejecutado y no cargado desde el archivo\n",
    "#Por eso hay inconsistencias en los caracteres (el mapeo se ha desordenado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Segundo intento de ENTRENAMIENTO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 0\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 48s 3ms/step - loss: 2.7454\n",
      "un the he the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "\n",
      "Epoch: 1\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 2.1540\n",
      "/ the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the store the stor\n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.9082\n",
      "2 the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the store the streat of the \n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.7526\n",
      "But the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang the strang t\n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.6431\n",
      "ut the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat the streat th\n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.5638\n",
      "Eng of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the\n",
      "\n",
      "Epoch: 6\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.5047\n",
      "Come the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the s\n",
      "\n",
      "Epoch: 7\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.4575\n",
      "Ring the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the stream of the s\n",
      "\n",
      "Epoch: 8\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.4196\n",
      "6 the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of the stone the stars of th\n",
      "\n",
      "Epoch: 9\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 48s 3ms/step - loss: 1.3875\n",
      "Ores and the streams of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of \n",
      "\n",
      "Epoch: 10\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.3600\n",
      "he stood the stars of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone of the stone \n",
      "\n",
      "Epoch: 11\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.3357\n",
      "Ught and the stars of the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and the stars and th\n",
      "\n",
      "Epoch: 12\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.3139\n",
      "And the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the shadow of the sh\n",
      "\n",
      "Epoch: 13\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 45s 3ms/step - loss: 1.2939\n",
      "2 the stair of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stone\n",
      "\n",
      "Epoch: 14\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 45s 3ms/step - loss: 1.2748\n",
      "were the strange things were already from the streams of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stone\n",
      "\n",
      "Epoch: 15\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.2572\n",
      "Sam stood up and stood and strong and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and stood the hobbits and \n",
      "\n",
      "Epoch: 16\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.2403\n",
      "ze the stones of the trees and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong and strong \n",
      "\n",
      "Epoch: 17\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.2241\n",
      "I should like to be seen them to the ground and the stars of the Ents and the others of the Ents and the others of the Ents and the others of the Ents and the stars and the stars are coming on the ground and the stars are the stars and the stars are the stars and the stars are the stars and the stars are the stars and the stars are the stars and the stars are the stars and the stars are the stars and the stars are the stars and the stars are the stars and the stars are the stars and the stars ar\n",
      "\n",
      "Epoch: 18\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.2086\n",
      "perhaps we shall see the way of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the st\n",
      "\n",
      "Epoch: 19\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.1933\n",
      "Not the hobbits saw that they had not seen the hills and the stairs of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stone\n",
      "\n",
      "Epoch: 20\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.1781\n",
      "e was a great deal of the wood, and then he stood up and stroke away from the rock of the stone with a strange place where the wood was still slowly in the stony slopes of the stones, and some sleep strange packs of the stones of the forest of the wood, and the world of the world came to the ground. The stones were great as a strange sword and shadow of the wood and strange them to the stream the shadow of the forest of the wood and strange them to the stream the shadow of the forest of the wood\n",
      "\n",
      "Epoch: 21\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 47s 3ms/step - loss: 1.1638\n",
      ") the stars were still slowly in the stars and the shadow of the stony hall of the star of the stony wall and then suddenly they were already better than the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mountains of the mo\n",
      "\n",
      "Epoch: 22\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.1495\n",
      "The mountains of the trees and the storm of the story was still strangely there was a great stone slopes of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of the story of th\n",
      "\n",
      "Epoch: 23\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.1351\n",
      "_ the walls of Mordor, and the water ran up the stream that he had not seen the wind of a long shadow of the wall, and they came and stranger of the stream that he had not seen the wind of a long shadow of the wall, and they came and stranger of the stream that he had not seen the wind of a long shadow of the wall, and they came and stranger of the stream that he had not seen the wind of a long shadow of the wall, and they came and stranger of the stream that he had not seen the wind of a long s\n",
      "\n",
      "Epoch: 24\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.1210\n",
      "\\ the stream that he seemed to be seen on the ground and crawled away, and the light was close at hand. The horn of the mountains were still and stream that he was a great strange pale and stream the stream that he seemed to be seen on the ground and crawled away, and the light was close at hand. The horn of the mountains were still and stream that he was a great strange pale and stream the stream that he seemed to be seen on the ground and crawled away, and the light was close at hand. The horn\n",
      "\n",
      "Epoch: 25\n",
      "\n",
      "Epoch 1/1\n",
      "16443/16443 [==============================] - 46s 3ms/step - loss: 1.1075\n",
      "But the sound of the world should have been the wild of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones of the stones\n",
      "\n",
      "Epoch: 26\n",
      "\n",
      "Epoch 1/1\n",
      "12450/16443 [=====================>........] - ETA: 11s - loss: 1.0898"
     ]
    }
   ],
   "source": [
    "nb_epoch = 0\n",
    "while True:\n",
    "    print('\\n\\nEpoch: {}\\n'.format(nb_epoch))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=1)\n",
    "    nb_epoch += 1\n",
    "    generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "    if nb_epoch % 10 == 0:\n",
    "        model.save_weights('checkpoint_layer_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRUEBA correcta de un checkpoint anterior**\n",
    "<br>Si instancian el modelo y sus parametros (ejecutando algunas celdas preliminares), y tienen los 2 archivos requeridos (.pickle y .hdf5) pueden generar el texto. \n",
    "<br>En el ejemplo de LOTR: `VOCAB_SIZE = 84` (si desean probarlo, se adjuntar los pesos y el diccionario, pero no el texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could see that a wind was chilled and leaned from his head and earth. Twitching the horse shifted in a could began to spread from him or was to see the wall and staring and half-train. It became green spider. There was no sign on the ground and westward.\n",
      "\n",
      "'Why so, what he was good! ' said Sam. 'So master to see you say thar,' said Aragorn.\n",
      "\n",
      "'Come back!' said Treebeard. 'But I have no with the fountain of the Marks on the fire to watce the deed of your own friends chooney things to think that you\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cuidar de no reemplazar el pickle original\n",
    "with open('ix_to_char.pickle', 'rb') as handle:\n",
    "    ix_to_char = pickle.load(handle)\n",
    "    \n",
    "WEIGHTS = \"checkpoint_layer_2_hidden_250_epoch_60.hdf5\"\n",
    "# Loading the trained weights\n",
    "model.load_weights(WEIGHTS)\n",
    "generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
