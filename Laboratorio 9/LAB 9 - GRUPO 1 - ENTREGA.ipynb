{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1O-_VvdXw4h"
   },
   "source": [
    "### Contenido del Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter Sentiment140: Tweets related to brands/keywords. Website includes papers and research ideas. (77 MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfUXi7sqXw4j"
   },
   "source": [
    "The data is a CSV with emoticons removed. Data file format has 6 fields:\n",
    "- **0** - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "- **1** - the id of the tweet (2087)\n",
    "- **2** - the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "- **3** - the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "- **4** - the user that tweeted (robotickilldozr)\n",
    "- **5** - the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puLzRFPyXw4m"
   },
   "source": [
    "If you use this data, please cite Sentiment140 as your source.\n",
    "- **Link** : http://help.sentiment140.com/for-students/\n",
    "- **CSV** : https://docs.google.com/file/d/0B04GJPshIjmPRnZManQwWEdTZjg/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WHdlF5c7Xw4o"
   },
   "source": [
    "## 1. Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dyCnDoXoXw4s",
    "outputId": "c62d29d2-ed47-43e8-b6ae-483856c87bec"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "p = 0.95  # 1% of the lines\n",
    "# keep the header, then take only 1% of lines\n",
    "\n",
    "header_names = [\"polarity\",\"id\",\"date\",\"query\",\"user\",\"tweet\"]\n",
    "df = pd.read_csv(\"trainingandtestdata/training.1600000.processed.noemoticon.csv\", sep=',', names = header_names, encoding=\"ISO-8859-1\",header=None, skiprows=lambda i: i>0 and random.random() < p)\n",
    "test = pd.read_csv(\"trainingandtestdata/testdata.manual.2009.06.14.csv\", sep=',', names = header_names, encoding=\"ISO-8859-1\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_size(df):\n",
    "    \"\"\"Return the size of a DataFrame in Megabyes\"\"\"\n",
    "    total = 0.0\n",
    "    for col in df:\n",
    "        total += df[col].nbytes\n",
    "    return total/1048576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de archivo: 3.64288330078125 MB\n"
     ]
    }
   ],
   "source": [
    "print('Tamaño de archivo:',df_size(df), 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "migD5PMXXw44"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814119</td>\n",
       "      <td>Mon Apr 06 22:20:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cooliodoc</td>\n",
       "      <td>@angry_barista I baked you a cake but I ated it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467817225</td>\n",
       "      <td>Mon Apr 06 22:21:27 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>crosland_12</td>\n",
       "      <td>@cocomix04 ill tell ya the story later  not a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467837762</td>\n",
       "      <td>Mon Apr 06 22:26:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Dogbook</td>\n",
       "      <td>Emily will be glad when Mommy is done training...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467838189</td>\n",
       "      <td>Mon Apr 06 22:26:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>missannabanana</td>\n",
       "      <td>laying in bed with no voice..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467840386</td>\n",
       "      <td>Mon Apr 06 22:27:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>MadameCrow</td>\n",
       "      <td>I am in pain. My back and sides hurt. Not to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467843215</td>\n",
       "      <td>Mon Apr 06 22:28:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mikecogh</td>\n",
       "      <td>@ozesteph1992 Shame to hear this Stephan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467854345</td>\n",
       "      <td>Mon Apr 06 22:31:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ace587</td>\n",
       "      <td>@pinkserendipity yes sprint has 4g only in bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467857975</td>\n",
       "      <td>Mon Apr 06 22:32:06 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>szrhnds602</td>\n",
       "      <td>Borders closed at 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467859025</td>\n",
       "      <td>Mon Apr 06 22:32:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LeeseEllen</td>\n",
       "      <td>So many channels.... yet so so boring... lazy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity          id                          date     query  \\\n",
       "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1         0  1467814119  Mon Apr 06 22:20:40 PDT 2009  NO_QUERY   \n",
       "2         0  1467817225  Mon Apr 06 22:21:27 PDT 2009  NO_QUERY   \n",
       "3         0  1467837762  Mon Apr 06 22:26:48 PDT 2009  NO_QUERY   \n",
       "4         0  1467838189  Mon Apr 06 22:26:54 PDT 2009  NO_QUERY   \n",
       "5         0  1467840386  Mon Apr 06 22:27:31 PDT 2009  NO_QUERY   \n",
       "6         0  1467843215  Mon Apr 06 22:28:18 PDT 2009  NO_QUERY   \n",
       "7         0  1467854345  Mon Apr 06 22:31:09 PDT 2009  NO_QUERY   \n",
       "8         0  1467857975  Mon Apr 06 22:32:06 PDT 2009  NO_QUERY   \n",
       "9         0  1467859025  Mon Apr 06 22:32:22 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1        cooliodoc   @angry_barista I baked you a cake but I ated it   \n",
       "2      crosland_12  @cocomix04 ill tell ya the story later  not a ...  \n",
       "3          Dogbook  Emily will be glad when Mommy is done training...  \n",
       "4   missannabanana                     laying in bed with no voice..   \n",
       "5       MadameCrow  I am in pain. My back and sides hurt. Not to m...  \n",
       "6         mikecogh          @ozesteph1992 Shame to hear this Stephan   \n",
       "7           ace587  @pinkserendipity yes sprint has 4g only in bal...  \n",
       "8       szrhnds602                              Borders closed at 10   \n",
       "9       LeeseEllen  So many channels.... yet so so boring... lazy ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79580, 6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos de que todas las columnas estar cargando correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity\n",
      "id\n",
      "date\n",
      "query\n",
      "user\n",
      "tweet\n"
     ]
    }
   ],
   "source": [
    "for x in df:\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = df['polarity'].values\n",
    "tweets = df['tweet'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "print (category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos funcion de carga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_datos(porcentaje):\n",
    "    import pandas as pd\n",
    "    import random\n",
    "\n",
    "    p = porcentaje  # 1% of the lines\n",
    "    # keep the header, then take only 1% of lines\n",
    "    header_names = [\"polarity\",\"id\",\"date\",\"query\",\"user\",\"tweet\"]\n",
    "    df = pd.read_csv(\"trainingandtestdata/training.1600000.processed.noemoticon.csv\", sep=',', names = header_names, encoding=\"ISO-8859-1\",header=None, skiprows=lambda i: i>0 and random.random() < p)\n",
    "    test = pd.read_csv(\"trainingandtestdata/testdata.manual.2009.06.14.csv\", sep=',', names = header_names, encoding=\"ISO-8859-1\",header=None)\n",
    "    category = df['polarity'].values\n",
    "    tweets = df['tweet'].values\n",
    "    return df,test,category,tweets\n",
    "#    df = pd.read_csv(\"trainingandtestdata/training.1600000.processed.noemoticon.csv\", sep=',', names = header_names, encoding=\"ISO-8859-1\",header=None,nrows = 10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    39797\n",
      "0    39783\n",
      "Name: polarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideramos válido el apóstrofe (') debido a que este es parte de varios *stopwords*, concepto que será explicado luego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tweets_alphanum=[]\n",
    "for i in tweets:\n",
    "    x = re.sub(r\"http\\S+|www\\S+\",\" \",i)\n",
    "    x = re.sub(r\"[^\\w'@ ]|@\\W+\",\" \",x)\n",
    "    x = re.sub(r\"^(\\d ?)+| (\\d ?)+\",\" \",x)\n",
    "    tweets_alphanum.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_lower=[]\n",
    "for i in tweets_alphanum:\n",
    "    tweets_lower.append(i.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los *stopwords* o palabras vacías son términos que por lo general no guardan mucha información relevante para el procesamiento de la información textual (si no se analiza a nivel contextual). Por ello, para este caso, se removerán los stopwords de los títulos. Indique la cantidad de stopwords que está removiendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alirapal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords_eng = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de stopwords removidos: 430167\n"
     ]
    }
   ],
   "source": [
    "tweets_lower_clean=[]\n",
    "count = 0\n",
    "for tweet in tweets_lower:\n",
    "    tweet_clean = ''\n",
    "    for token in tweet.split():\n",
    "        if token not in stopwords_eng:\n",
    "            tweet_clean += token\n",
    "            tweet_clean += ' '\n",
    "        else:\n",
    "            count += 1    \n",
    "        tweets_lower_clean.append(tweet_clean)\n",
    "print(\"Cantidad de stopwords removidos:\",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los tweets con solamente carácteres alfanuméricos en minuscula limpios de stopwords, se procederá a analizar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tokens mas comunes: [(\"i'm\", 67099), ('get', 36750), ('like', 35372), ('quot', 35331), ('good', 34730), ('got', 33796), ('day', 33264), ('going', 32864), ('go', 29578), ('know', 26618)]\n",
      "Numero de tokens: 5343919\n",
      "Tamaño del vocabulario: 80311\n"
     ]
    }
   ],
   "source": [
    "import collections as co\n",
    "tokens_list = []\n",
    "c = co.Counter() \n",
    "\n",
    "for text in tweets_lower_clean:\n",
    "    tokens = text.split()\n",
    "    c.update(tokens)\n",
    "\n",
    "print('10 tokens mas comunes:',c.most_common(10))\n",
    "print('Numero de tokens:',sum(c.values()))\n",
    "print('Tamaño del vocabulario:',len(c))\n",
    "## Observacion: También se puede calcular el tamaño del vocabulario con\n",
    "## len(list(set(\" \".join(tweets_lower_clean).split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos funciones de preprocesamiento y limpieza de datos resumiendo todo el análisis previo, como también una función de cálculo de estádisticas según el diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar(tweets):\n",
    "    tweets_alphanum=[]\n",
    "    for i in tweets:\n",
    "        x = re.sub(r\"http\\S+|www\\S+\",\" \",i)\n",
    "        x = re.sub(r\"[^\\w'@ ]|@\\W+|[^a-zA-Z]'[^a-zA-Z]\",\" \",x)\n",
    "        x = re.sub(r\"^(\\d ?)+| (\\d ?)+\",\" \",x)\n",
    "        tweets_alphanum.append(x)\n",
    "    tweets_lower=[]\n",
    "    for tweet in tweets_alphanum:\n",
    "        tweets_lower.append(tweet.lower())\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    stopwords_eng = nltk.corpus.stopwords.words('english')\n",
    "    tweets_lower_clean=[]\n",
    "    for tweet in tweets_lower:\n",
    "        tweet_clean = ''\n",
    "        for token in tweet.split():\n",
    "            if token not in stopwords_eng:\n",
    "                tweet_clean += token + ' '\n",
    "        tweets_lower_clean.append(tweet_clean)\n",
    "    import collections as co\n",
    "    diccionario = co.Counter() \n",
    "    for text in tweets_lower_clean:\n",
    "        tokens = text.split()\n",
    "        diccionario.update(tokens)\n",
    "    return diccionario, tweets_lower_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estadisticas(diccionario):\n",
    "    print('10 tokens más comunes:',diccionario.most_common(10))        \n",
    "    print('Número de tokens:',sum(diccionario.values()))\n",
    "    print('Tamaño del vocabulario:',len(diccionario))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deben mostrar un reporte de clasificación (accuracy) con diversos algoritmos de aprendizaje supervisado tradicionales (adicionales a los mostrados en clase) usando los siguientes vectores:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1**. Bag of Words (booleano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1042116, 10000)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "text = np.array(tweets_lower_clean)\n",
    "txd_matrix = CountVectorizer(binary=True,max_features = 10000).fit_transform(text)\n",
    "tf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos función que nos devuelve la matriz Bag of Words (booleano)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bag_Of_Words(tweets_lower_clean, max_words_dictionary = None):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    import numpy as np\n",
    "    text = np.array(tweets_lower_clean)\n",
    "    txd_matrix = CountVectorizer(binary=True,max_features = max_words_dictionary).fit_transform(text)\n",
    "    return txd_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2** TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_1 = TfidfVectorizer(max_features=10000).fit_transform(tweets_lower_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos función que nos devuelve la matriz TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(tweets_lower_clean, max_words_dictionary = None):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    X_1 = TfidfVectorizer(stop_words='english',max_features=10000).fit_transform(tweets_lower_clean)\n",
    "    return (X_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **3** Word Vectors (embeddings) de spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalar spacy con las instrucciones en el siguiente link:\n",
    "<br>Spacy: https://spacy.io/usage/\n",
    "<br>Package: https://spacy.io/usage/models#section-available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deben instalar el modelo \"en_core_web_lg\"\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3947815"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Importante: No sobrepasar lons 1 000 000 de caracteres, por lo cual hay que comprobar con un len(text)\n",
    "diccionario, tweets_lower_clean = preprocesar(tweets)\n",
    "text = ''.join(tweets_lower_clean)\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 3947815 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-d8904c9b4b7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets_lower_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable)\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             raise ValueError(Errors.E088.format(length=len(text),\n\u001b[1;32m--> 345\u001b[1;33m                                                 max_length=self.max_length))\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E088] Text of length 3947815 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos función que nos devuelve la matriz Word Vectors (embeddings) de spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word_Vectors(tweets_lower_clean, max_words_dictionary = None):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se define una función de ayuda para ejecutar un modelo y que se imprima el resultado de clasificación (accuracy) mediante validación cruzada (k_fold=5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79580,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(category)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(clf, X, y):\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    print(\"%s accuracy: %0.2f (+/- %0.2f)\" % \\\n",
    "          (str(clf.__class__).split('.')[-1].replace('>','').replace(\"'\",''), \n",
    "          scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_models(X,y):\n",
    "    #run_model(LinearSVC(), X, y)\n",
    "    run_model(SGDClassifier(), X, y)\n",
    "    run_model(Perceptron(), X, y)\n",
    "    run_model(PassiveAggressiveClassifier(), X, y)\n",
    "    run_model(BernoulliNB(), X, y)\n",
    "    #run_model(MultinomialNB(), X, y)\n",
    "    #run_model(KNeighborsClassifier(), X, y)\n",
    "    #run_model(NearestCentroid(), X, y)\n",
    "    #run_model(RandomForestClassifier(n_estimators=100, max_depth=10), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora efectuamos la prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1**. Bag of Words (booleano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier accuracy: 0.76 (+/- 0.00)\n",
      "Perceptron accuracy: 0.69 (+/- 0.01)\n",
      "PassiveAggressiveClassifier accuracy: 0.69 (+/- 0.01)\n",
      "BernoulliNB accuracy: 0.76 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "text = np.array(tweets_lower_clean)\n",
    "txd_matrix = CountVectorizer(binary=True,max_features = 10000).fit_transform(text)\n",
    "run_models(txd_matrix,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2** TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier accuracy: 0.75 (+/- 0.01)\n",
      "Perceptron accuracy: 0.68 (+/- 0.01)\n",
      "PassiveAggressiveClassifier accuracy: 0.72 (+/- 0.01)\n",
      "BernoulliNB accuracy: 0.75 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "X_1 = TfidfVectorizer(stop_words='english',max_features=10000).fit_transform(tweets_lower_clean)\n",
    "run_models(X_1,y)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "LAB 9 - GRUPO 8.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
