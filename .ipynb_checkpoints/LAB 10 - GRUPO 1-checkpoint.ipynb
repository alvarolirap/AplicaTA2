{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación de Librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando Anaconda Prompt se debe usar los siguientes comandos para importar la librería de Keras.\n",
    "<br>Link: https://medium.com/@pushkarmandot/installing-tensorflow-theano-and-keras-in-spyder-84de7eb0f0df\n",
    "<br>TensorFlow: https://www.tensorflow.org/install/\n",
    "<br>Link: https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/\n",
    "\n",
    "```conda create -n tensorflow-gpu pip python=3.5```\n",
    "<br>```conda activate tensorflow-gpu```\n",
    "<br>```conda install keras ```\n",
    "\n",
    "Otra opción es la siguiente:\n",
    "<br>Entrar en modo administrador a Anaconda Prompt e introducir los siguientes comandos.\n",
    "<br>```conda update conda ```\n",
    "<br>```conda install keras ```\n",
    "\n",
    "Añadir en el path las librerias importadas si es que estás en Windows:\n",
    "\n",
    "``` set path=%PATH%;C:\\Users\\Alvaro\\AppData\\Roaming\\Python\\Python35\\Scripts ```\n",
    "\n",
    "Si está usando Linux, usar los siguientes comandos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install cython --user\n",
    "#!pip install --force-reinstall regex==2017.04.5\n",
    "#!pip install pathlib --user\n",
    "#!pip install msgpack --user\n",
    "!pip install tensorflow-gpu --user\n",
    "!pip install keras --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebo la correcta importación de librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alirapal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TF!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant(\"Hello, TF!\")\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "print(sess.run(a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Archivo de texto \n",
    "DATA_DIR = \"./warpeace_input.txt\" \n",
    "#Modificar BATCH_SIZE o HIDDEN_DIM en caso tengan problemas de memoria\n",
    "BATCH_SIZE = 50 \n",
    "HIDDEN_DIM = 250 #500\n",
    "#Parametro para longitud de secuencia a analizar\n",
    "SEQ_LENGTH = 50 \n",
    "#Parametro para cargar un pesos previamente entrenados (checkpoint)\n",
    "WEIGHTS = '' \n",
    "\n",
    "#Parametro para indicar cuantos caracteres generar en cada prueba\n",
    "GENERATE_LENGTH = 500 \n",
    "#Parametros para la red neuronal\n",
    "LAYER_NUM = 2 \n",
    "NB_EPOCH = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función A:\n",
    "<br>(1) Carga de un archivo de texto, (2) Construcción de estructuras de entrada y salida de la red**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for preparing the training data\n",
    "def load_data(data_dir, seq_length):\n",
    "    #Carga del archivo\n",
    "    data = open(data_dir, 'r').read()\n",
    "    #Caracteres unicos\n",
    "    chars = list(set(data))\n",
    "    VOCAB_SIZE = len(chars)\n",
    "\n",
    "    print('Data length: {} characters'.format(len(data)))\n",
    "    print('Vocabulary size: {} characters'.format(VOCAB_SIZE))\n",
    "    print(chars)\n",
    "    \n",
    "    #Indexacion de los caracteres\n",
    "    ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "    char_to_ix = {char:ix for ix, char in enumerate(chars)}\n",
    "    \n",
    "    #Estructuras de entrada y salida\n",
    "    NUMBER_OF_SEQ = int(len(data)/seq_length)\n",
    "    print('Number of sequences: {}'.format(NUMBER_OF_SEQ))\n",
    "    X = np.zeros((NUMBER_OF_SEQ, seq_length, VOCAB_SIZE))\n",
    "    y = np.zeros((NUMBER_OF_SEQ, seq_length, VOCAB_SIZE))\n",
    "    \n",
    "    for i in range(0, NUMBER_OF_SEQ):\n",
    "        #LLenado de la estructura de entrada X\n",
    "        X_sequence = data[i*seq_length:(i+1)*seq_length]\n",
    "        X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n",
    "        #one-hot-vector (input)\n",
    "        input_sequence = np.zeros((seq_length, VOCAB_SIZE))  \n",
    "        #uso del diccionario para completar el one-hot-vector\n",
    "        for j in range(seq_length):\n",
    "            input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "            X[i] = input_sequence\n",
    "            \n",
    "        #Llenado de la estructura de salida y\n",
    "        y_sequence = data[i*seq_length+1:(i+1)*seq_length+1]\n",
    "        y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
    "        #one-hot-vector (output)\n",
    "        target_sequence = np.zeros((seq_length, VOCAB_SIZE))\n",
    "        #uso del diccionario para completar el one-hot-vector\n",
    "        for j in range(seq_length):\n",
    "            target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "            y[i] = target_sequence\n",
    "            \n",
    "    return X, y, VOCAB_SIZE, ix_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función B:\n",
    "<br>Generación de textos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for generating text\n",
    "def generate_text(model, length, vocab_size, ix_to_char):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y Prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uso de la Función A: carga de los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 3196232 characters\n",
      "Vocabulary size: 86 characters\n",
      "['.', '2', 'v', 'a', 'B', '!', '(', 'W', 'u', 'q', 'o', 'U', 'b', 'D', 'z', 'f', 'n', '¤', ' ', 'r', '?', 'C', 't', '9', '/', 'k', 'Q', 'S', '\\xa0', 'x', 'Z', 'L', '»', '©', 'N', 'K', '*', 'ï', \"'\", 'y', 'I', '¿', 'l', ';', ':', 's', 'V', 'A', 'g', 'G', '0', 'i', 'Ã', '4', 'Y', 'M', '=', 'H', '1', 'w', 'j', '6', '\"', 'P', '3', '-', '5', 'd', 'm', 'J', ',', '8', 'h', 'T', 'F', '\\n', 'c', 'p', ')', 'X', 'ª', 'O', 'R', 'e', '7', 'E']\n",
      "Number of sequences: 63924\n"
     ]
    }
   ],
   "source": [
    "# Creating training data\n",
    "X, y, VOCAB_SIZE, ix_to_char = load_data(DATA_DIR, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Es importante guardar el diccionario `ix_to_char` en un archivo binario. Este debe ser cargado cada vez que se quiera retomar el entrenamiento o generar texto a partir de un checkpoint, debido a que el orden de los caracteres en el diccionario podría modificarse (no es un orden fijo)**\n",
    "<br>**NO MODIFICAR ESTE PICKLE AL REINICIAR EL NOTEBOOK PARA PROBAR CHECKPOINTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALERTA: NO EJECUTAR LAS 3 SIGUIENTES LÍNEA SI ES QUE YA EXISTEN CHECKPOINTS Y EL IX_TO_CHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No modificar el pickle al reiniciar el cuaderno de trabajo para probar checkpoints previos\n",
    "with open('ix_to_char.pickle', 'wb') as handle:\n",
    "    pickle.dump(ix_to_char, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '.', 1: '2', 2: 'v', 3: 'a', 4: 'B', 5: '!', 6: '(', 7: 'W', 8: 'u', 9: 'q', 10: 'o', 11: 'U', 12: 'b', 13: 'D', 14: 'z', 15: 'f', 16: 'n', 17: '¤', 18: ' ', 19: 'r', 20: '?', 21: 'C', 22: 't', 23: '9', 24: '/', 25: 'k', 26: 'Q', 27: 'S', 28: '\\xa0', 29: 'x', 30: 'Z', 31: 'L', 32: '»', 33: '©', 34: 'N', 35: 'K', 36: '*', 37: 'ï', 38: \"'\", 39: 'y', 40: 'I', 41: '¿', 42: 'l', 43: ';', 44: ':', 45: 's', 46: 'V', 47: 'A', 48: 'g', 49: 'G', 50: '0', 51: 'i', 52: 'Ã', 53: '4', 54: 'Y', 55: 'M', 56: '=', 57: 'H', 58: '1', 59: 'w', 60: 'j', 61: '6', 62: '\"', 63: 'P', 64: '3', 65: '-', 66: '5', 67: 'd', 68: 'm', 69: 'J', 70: ',', 71: '8', 72: 'h', 73: 'T', 74: 'F', 75: '\\n', 76: 'c', 77: 'p', 78: ')', 79: 'X', 80: 'ª', 81: 'O', 82: 'R', 83: 'e', 84: '7', 85: 'E'}\n"
     ]
    }
   ],
   "source": [
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63924, 50, 86) (63924, 50, 86) 86\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de la RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and compiling the Network\n",
    "model = Sequential()\n",
    "\n",
    "#Añadiendo las capas LSTM\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "#Añadiendo la operacion de salida\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#\"Compilando\" = instanciando la RNN con su función de pérdida y optimización\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P))ªª//((//g35555f5\n",
      "AAA''''mmmNNNNccc!!!uuuufcc!!uuufcc!!uuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuu"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"P))ªª//((//g35555f5\\nAAA''''mmmNNNNccc!!!uuuufcc!!uuufcc!!uuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuumfccc!!uuuum\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate some sample before training to know how bad it is!\n",
    "generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se cargan los pesos (y el diccionario de los one-hot-vectors) en caso haya habido un entrenamiento previo**\n",
    "<br>WEIGHTS debe tener el valor del nombre del archivo de \"checkpoint\" guardado. Por ejemplo:\n",
    "<br>```WEIGHTS = \"checkpoint_layer_2_hidden_250_epoch_60.hdf5\"```\n",
    "# LA SIGUIENTE LÍNEA SE EJECUTARÁ CADA VEZ QUE QUIERA ENTRENAR MÁS AL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan los pesos de un entrenamiento previo (si se desea restaurar una ejecucion)\n",
    "#Se calcula el numero de epocas en base al nombre del archivo\n",
    "#Se carga el diccionario de caracteres (one-hot-vectors) para la generacion\n",
    "\n",
    "### WEIGHTS = \"checkpoint_layer_2_hidden_250_epoch_60.hdf5\" \n",
    "\n",
    "if not WEIGHTS == '':\n",
    "    model.load_weights(WEIGHTS)\n",
    "    nb_epoch = int(WEIGHTS[WEIGHTS.rfind('_') + 1:WEIGHTS.find('.')])\n",
    "    with open('ix_to_char.pickle', 'rb') as handle:\n",
    "        ix_to_char = pickle.load(handle)\n",
    "else:\n",
    "    #Si se va a empezar de 0:\n",
    "    nb_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 0\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 805s 13ms/step - loss: 2.1058\n",
      "ver the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of th\n",
      "\n",
      "Epoch: 1\n",
      "\n",
      "Epoch 1/1\n",
      "63924/63924 [==============================] - 766s 12ms/step - loss: 1.5385\n",
      "  the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the same to the sa\n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Epoch 1/1\n",
      "56500/63924 [=========================>....] - ETA: 1:35 - loss: 1.4007"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c7005652fdf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\nEpoch: {}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#Ajuste del modelo, y entrenamiento de 1 epoca\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mnb_epoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#Generacion de un texto al final de la epoca\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training if there is no trained weights specified\n",
    "\n",
    "#Esta es la iteración importante\n",
    "#Pueden cambiar la condición para que termine en un determinado numero de epochs.\n",
    "while True:\n",
    "    print('\\n\\nEpoch: {}\\n'.format(nb_epoch))\n",
    "    #Ajuste del modelo, y entrenamiento de 1 epoca\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=1)\n",
    "    nb_epoch += 1\n",
    "    #Generacion de un texto al final de la epoca\n",
    "    generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "    #Pueden modificar esto para tener más checkpoints\n",
    "    if nb_epoch % 10 == 0:\n",
    "        model.save_weights('checkpoint_layer_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch))\n",
    "    if nb_epoch == 50\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de texto\n",
    "Si instancian el modelo y sus parametros (ejecutando algunas celdas preliminares), y tienen los 2 archivos requeridos (.pickle y .hdf5) pueden generar el texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si es que se va a cargar un checkpoint, ejecutar las siguientes dos líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 3196232 characters\n",
      "Vocabulary size: 86 characters\n",
      "['.', '2', 'v', 'a', 'B', '!', '(', 'W', 'u', 'q', 'o', 'U', 'b', 'D', 'z', 'f', 'n', '¤', ' ', 'r', '?', 'C', 't', '9', '/', 'k', 'Q', 'S', '\\xa0', 'x', 'Z', 'L', '»', '©', 'N', 'K', '*', 'ï', \"'\", 'y', 'I', '¿', 'l', ';', ':', 's', 'V', 'A', 'g', 'G', '0', 'i', 'Ã', '4', 'Y', 'M', '=', 'H', '1', 'w', 'j', '6', '\"', 'P', '3', '-', '5', 'd', 'm', 'J', ',', '8', 'h', 'T', 'F', '\\n', 'c', 'p', ')', 'X', 'ª', 'O', 'R', 'e', '7', 'E']\n",
      "Number of sequences: 63924\n"
     ]
    }
   ],
   "source": [
    "# Creating training data\n",
    "X, y, VOCAB_SIZE, ix_to_char = load_data(DATA_DIR, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and compiling the Network\n",
    "model = Sequential()\n",
    "\n",
    "#Añadiendo las capas LSTM\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "#Añadiendo la operacion de salida\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#\"Compilando\" = instanciando la RNN con su función de pérdida y optimización\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u(ROFoPZFSZ'PR"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "85",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-3038684d4d67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Loading the trained weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWEIGHTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mgenerate_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGENERATE_LENGTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mix_to_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-659268d180ae>\u001b[0m in \u001b[0;36mgenerate_text\u001b[1;34m(model, length, vocab_size, ix_to_char)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mix_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0my_char\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mix_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 85"
     ]
    }
   ],
   "source": [
    "#Cuidar de no reemplazar el pickle original\n",
    "with open('ix_to_char.pickle', 'rb') as handle:\n",
    "    ix_to_char = pickle.load(handle)\n",
    "    \n",
    "WEIGHTS = \"checkpoint_layer_2_hidden_250_epoch_50.hdf5\"\n",
    "# Loading the trained weights\n",
    "model.load_weights(WEIGHTS)\n",
    "generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char)\n",
    "print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
